{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b42de981",
   "metadata": {},
   "source": [
    "# TCGA-GBMLGG Analysis Demo\n",
    "\n",
    "- **Cohort**: Focuses on the TCGA-GBMLGG dataset, a vital resource merging Glioblastoma Multiforme (GBM) and Lower-Grade Glioma (LGG).\n",
    "- **Goal**: Perform histological subtype classification.\n",
    "- **Prediction Target**: Predict whether a tumor is an `astrocytoma`, `oligodendroglioma`, or `oligoastrocytoma` based on its multi-omics profile.\n",
    "\n",
    "**Data Source:** Broad Institute FireHose (http://firebrowse.org/?cohort=GBMLGG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "027dc20c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TCGA-06-0675-11</th>\n",
       "      <th>TCGA-06-0678-11</th>\n",
       "      <th>TCGA-06-0680-11</th>\n",
       "      <th>TCGA-06-0681-11</th>\n",
       "      <th>TCGA-06-AABW-11</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gene</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>hsa-let-7a-1</th>\n",
       "      <td>12.847399</td>\n",
       "      <td>13.789578</td>\n",
       "      <td>13.603454</td>\n",
       "      <td>13.346797</td>\n",
       "      <td>13.545128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hsa-let-7a-2</th>\n",
       "      <td>13.850719</td>\n",
       "      <td>14.792970</td>\n",
       "      <td>14.597877</td>\n",
       "      <td>14.344260</td>\n",
       "      <td>14.554888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hsa-let-7a-3</th>\n",
       "      <td>12.873946</td>\n",
       "      <td>13.810832</td>\n",
       "      <td>13.611074</td>\n",
       "      <td>13.364372</td>\n",
       "      <td>13.583039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              TCGA-06-0675-11  TCGA-06-0678-11  TCGA-06-0680-11  \\\n",
       "gene                                                              \n",
       "hsa-let-7a-1        12.847399        13.789578        13.603454   \n",
       "hsa-let-7a-2        13.850719        14.792970        14.597877   \n",
       "hsa-let-7a-3        12.873946        13.810832        13.611074   \n",
       "\n",
       "              TCGA-06-0681-11  TCGA-06-AABW-11  \n",
       "gene                                            \n",
       "hsa-let-7a-1        13.346797        13.545128  \n",
       "hsa-let-7a-2        14.344260        14.554888  \n",
       "hsa-let-7a-3        13.364372        13.583039  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(548, 531)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TCGA-02-0047-01</th>\n",
       "      <th>TCGA-02-0055-01</th>\n",
       "      <th>TCGA-02-2483-01</th>\n",
       "      <th>TCGA-02-2485-01</th>\n",
       "      <th>TCGA-02-2486-01</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gene</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>?|100133144</th>\n",
       "      <td>1.619742</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.559100</td>\n",
       "      <td>3.999567</td>\n",
       "      <td>2.475344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>?|100134869</th>\n",
       "      <td>2.757258</td>\n",
       "      <td>3.972445</td>\n",
       "      <td>3.801138</td>\n",
       "      <td>3.902759</td>\n",
       "      <td>2.264506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>?|10357</th>\n",
       "      <td>5.773564</td>\n",
       "      <td>4.972440</td>\n",
       "      <td>5.915141</td>\n",
       "      <td>6.520796</td>\n",
       "      <td>5.966629</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             TCGA-02-0047-01  TCGA-02-0055-01  TCGA-02-2483-01  \\\n",
       "gene                                                             \n",
       "?|100133144         1.619742              NaN         1.559100   \n",
       "?|100134869         2.757258         3.972445         3.801138   \n",
       "?|10357             5.773564         4.972440         5.915141   \n",
       "\n",
       "             TCGA-02-2485-01  TCGA-02-2486-01  \n",
       "gene                                           \n",
       "?|100133144         3.999567         2.475344  \n",
       "?|100134869         3.902759         2.264506  \n",
       "?|10357             6.520796         5.966629  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(20115, 685)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TCGA-06-0125-01</th>\n",
       "      <th>TCGA-06-0125-02</th>\n",
       "      <th>TCGA-06-0152-01</th>\n",
       "      <th>TCGA-06-0152-02</th>\n",
       "      <th>TCGA-06-0171-01</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hybridization REF</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Composite Element REF</th>\n",
       "      <td>Beta_Value</td>\n",
       "      <td>Beta_Value</td>\n",
       "      <td>Beta_Value</td>\n",
       "      <td>Beta_Value</td>\n",
       "      <td>Beta_Value</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A1BG</th>\n",
       "      <td>0.438986043005</td>\n",
       "      <td>0.565094788162</td>\n",
       "      <td>0.461699906718</td>\n",
       "      <td>0.534127262606</td>\n",
       "      <td>0.455267108058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A1CF</th>\n",
       "      <td>0.681141812896</td>\n",
       "      <td>0.724487443757</td>\n",
       "      <td>0.601439733092</td>\n",
       "      <td>0.632221318323</td>\n",
       "      <td>0.691054589549</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      TCGA-06-0125-01 TCGA-06-0125-02 TCGA-06-0152-01  \\\n",
       "Hybridization REF                                                       \n",
       "Composite Element REF      Beta_Value      Beta_Value      Beta_Value   \n",
       "A1BG                   0.438986043005  0.565094788162  0.461699906718   \n",
       "A1CF                   0.681141812896  0.724487443757  0.601439733092   \n",
       "\n",
       "                      TCGA-06-0152-02 TCGA-06-0171-01  \n",
       "Hybridization REF                                      \n",
       "Composite Element REF      Beta_Value      Beta_Value  \n",
       "A1BG                   0.534127262606  0.455267108058  \n",
       "A1CF                   0.632221318323  0.691054589549  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(20115, 685)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tcga-06-6391</th>\n",
       "      <th>tcga-19-a6j4</th>\n",
       "      <th>tcga-cs-6665</th>\n",
       "      <th>tcga-cs-6670</th>\n",
       "      <th>tcga-db-a4xc</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hybridization REF</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Composite Element REF</th>\n",
       "      <td>value</td>\n",
       "      <td>value</td>\n",
       "      <td>value</td>\n",
       "      <td>value</td>\n",
       "      <td>value</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>years_to_birth</th>\n",
       "      <td>44</td>\n",
       "      <td>68</td>\n",
       "      <td>51</td>\n",
       "      <td>43</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vital_status</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      tcga-06-6391 tcga-19-a6j4 tcga-cs-6665 tcga-cs-6670  \\\n",
       "Hybridization REF                                                           \n",
       "Composite Element REF        value        value        value        value   \n",
       "years_to_birth                  44           68           51           43   \n",
       "vital_status                     1            1            0            0   \n",
       "\n",
       "                      tcga-db-a4xc  \n",
       "Hybridization REF                   \n",
       "Composite Element REF        value  \n",
       "years_to_birth                  26  \n",
       "vital_status                     0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(14, 1110)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "root = Path(\"/home/vicente/Github/BioNeuralNet/GBMLGG\")\n",
    "\n",
    "mirna_raw = pd.read_csv(root/\"GBMLGG.miRseq_RPKM_log2.txt\", sep=\"\\t\",index_col=0,low_memory=False)                            \n",
    "rna_raw = pd.read_csv(root / \"GBMLGG.uncv2.mRNAseq_RSEM_normalized_log2.txt\", sep=\"\\t\",index_col=0,low_memory=False)\n",
    "meth_raw = pd.read_csv(root/\"GBMLGG.meth.by_mean.data.txt\", sep='\\t',index_col=0,low_memory=False)\n",
    "clinical_raw = pd.read_csv(root / \"GBMLGG.clin.merged.picked.txt\",sep=\"\\t\", index_col=0, low_memory=False)\n",
    "\n",
    "# display shapes and first few rows-columns of each file\n",
    "display(mirna_raw.iloc[:3,:5])\n",
    "display(mirna_raw.shape)\n",
    "\n",
    "display(rna_raw.iloc[:3,:5])\n",
    "display(meth_raw.shape)\n",
    "\n",
    "display(meth_raw.iloc[:3,:5])\n",
    "display(meth_raw.shape)\n",
    "\n",
    "display(clinical_raw.iloc[:3,:5])\n",
    "display(clinical_raw.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbacccaf",
   "metadata": {},
   "source": [
    "## Data Processing Summary\n",
    "\n",
    "1. **Transpose Data:** All raw data (miRNA, RNA, etc.) is flipped so rows represent patients and columns represent features.\n",
    "2. **Standardize Patient IDs:** Patient IDs in all tables are cleaned to the 12-character TCGA format (e.g., `TCGA-AB-1234`) for matching.\n",
    "3. **Handle Duplicates:** Duplicate patient rows are averaged in the omics data. The first entry is kept for duplicate patients in the clinical data.\n",
    "4. **Find Common Patients:** The script identifies the list of patients that exist in *all* datasets.\n",
    "5. **Subset Data:** All data tables are filtered down to *only* this common list of patients, ensuring alignment.\n",
    "6. **Extract Target:** The `histological_type` column is pulled from the processed clinical data to be used as the prediction target (y-variable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820583f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "miRNA (samples, features): (531, 548)\n",
      "RNA (samples, features): (701, 18328)\n",
      "Methylation (samples, features): (685, 20115)\n",
      "Clinical (samples, features): (1110, 14)\n",
      "\n",
      "Methylation shape: (658, 20115)\n",
      "RNA shape: (681, 18328)\n",
      "miRNA shape: (517, 548)\n",
      "Clinical shape: (1110, 14)\n",
      "\n",
      "Found: 511 patients across all data types.\n"
     ]
    }
   ],
   "source": [
    "mirna = mirna_raw.T\n",
    "rna = rna_raw.T\n",
    "meth = meth_raw.T\n",
    "clinical = clinical_raw.T\n",
    "\n",
    "print(f\"miRNA (samples, features): {mirna.shape}\")\n",
    "print(f\"RNA (samples, features): {rna.shape}\")\n",
    "print(f\"Methylation (samples, features): {meth.shape}\")\n",
    "print(f\"Clinical (samples, features): {clinical.shape}\")\n",
    "\n",
    "def trim_barcode(idx):\n",
    "    return idx.to_series().str.slice(0, 12)\n",
    "\n",
    "# standarized patient IDs across all files\n",
    "meth.index = trim_barcode(meth.index)\n",
    "rna.index = trim_barcode(rna.index)\n",
    "mirna.index = trim_barcode(mirna.index)\n",
    "clinical.index = clinical.index.str.upper()\n",
    "clinical.index.name = \"Patient_ID\"\n",
    "\n",
    "# convert all data to numeric, coercing errors to NaN\n",
    "meth = meth.apply(pd.to_numeric, errors='coerce')\n",
    "rna = rna.apply(pd.to_numeric, errors='coerce')\n",
    "mirna = mirna.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# for any duplicate columns in the omics data, we average their values\n",
    "meth = meth.groupby(meth.index).mean()\n",
    "rna = rna.groupby(rna.index).mean()\n",
    "mirna = mirna.groupby(mirna.index).mean()\n",
    "\n",
    "# for any duplicate rows in the clinical data, we keep the first occurrence\n",
    "clinical = clinical[~clinical.index.duplicated(keep='first')]\n",
    "\n",
    "print(f\"\\nMethylation shape: {meth.shape}\")\n",
    "print(f\"RNA shape: {rna.shape}\")\n",
    "print(f\"miRNA shape: {mirna.shape}\")\n",
    "print(f\"Clinical shape: {clinical.shape}\")\n",
    "\n",
    "for df in [meth, rna, mirna]:\n",
    "    df.columns = df.columns.str.replace(r\"\\?\", \"unknown_\", regex=True)\n",
    "    df.columns = df.columns.str.replace(r\"\\|\", \"_\", regex=True)\n",
    "    df.columns = df.columns.str.replace(\"-\", \"_\", regex=False)\n",
    "    df.columns = df.columns.str.replace(r\"_+\", \"_\", regex=True)\n",
    "    df.columns = df.columns.str.strip(\"_\")\n",
    "    \n",
    "    df.fillna(df.mean(), inplace=True)\n",
    "\n",
    "# to see which pateints are common across all data files\n",
    "common_patients = sorted(list(set(meth.index)&set(rna.index)&set(mirna.index)&set(clinical.index)))\n",
    "\n",
    "print(f\"\\nFound: {len(common_patients)} patients across all data types.\")\n",
    "\n",
    "# subset to only common patients\n",
    "meth_processed = meth.loc[common_patients]\n",
    "rna_processed= rna.loc[common_patients]\n",
    "mirna_processed = mirna.loc[common_patients]\n",
    "clinical_processed = clinical.loc[common_patients]\n",
    "\n",
    "# extract target labels from clinical data\n",
    "targets = clinical_processed['histological_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9c562ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>gene</th>\n",
       "      <th>hsa_let_7a_1</th>\n",
       "      <th>hsa_let_7a_2</th>\n",
       "      <th>hsa_let_7a_3</th>\n",
       "      <th>hsa_let_7b</th>\n",
       "      <th>hsa_let_7c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TCGA-CS-4938</th>\n",
       "      <td>12.622353</td>\n",
       "      <td>13.632728</td>\n",
       "      <td>12.651613</td>\n",
       "      <td>14.208930</td>\n",
       "      <td>14.376942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-CS-4941</th>\n",
       "      <td>11.809808</td>\n",
       "      <td>12.815815</td>\n",
       "      <td>11.820061</td>\n",
       "      <td>13.047853</td>\n",
       "      <td>11.955006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-CS-4942</th>\n",
       "      <td>11.113995</td>\n",
       "      <td>12.128618</td>\n",
       "      <td>11.165523</td>\n",
       "      <td>12.481790</td>\n",
       "      <td>11.858545</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "gene          hsa_let_7a_1  hsa_let_7a_2  hsa_let_7a_3  hsa_let_7b  hsa_let_7c\n",
       "TCGA-CS-4938     12.622353     13.632728     12.651613   14.208930   14.376942\n",
       "TCGA-CS-4941     11.809808     12.815815     11.820061   13.047853   11.955006\n",
       "TCGA-CS-4942     11.113995     12.128618     11.165523   12.481790   11.858545"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(511, 548)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>gene</th>\n",
       "      <th>unknown_100133144</th>\n",
       "      <th>unknown_100134869</th>\n",
       "      <th>unknown_10357</th>\n",
       "      <th>unknown_10431</th>\n",
       "      <th>unknown_155060</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TCGA-CS-4938</th>\n",
       "      <td>3.123352</td>\n",
       "      <td>4.507940</td>\n",
       "      <td>8.069184</td>\n",
       "      <td>9.724198</td>\n",
       "      <td>7.511790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-CS-4941</th>\n",
       "      <td>5.187819</td>\n",
       "      <td>4.404406</td>\n",
       "      <td>7.291745</td>\n",
       "      <td>8.608326</td>\n",
       "      <td>8.344526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-CS-4942</th>\n",
       "      <td>3.562316</td>\n",
       "      <td>3.462602</td>\n",
       "      <td>7.532460</td>\n",
       "      <td>9.279502</td>\n",
       "      <td>7.034985</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "gene          unknown_100133144  unknown_100134869  unknown_10357  \\\n",
       "TCGA-CS-4938           3.123352           4.507940       8.069184   \n",
       "TCGA-CS-4941           5.187819           4.404406       7.291745   \n",
       "TCGA-CS-4942           3.562316           3.462602       7.532460   \n",
       "\n",
       "gene          unknown_10431  unknown_155060  \n",
       "TCGA-CS-4938       9.724198        7.511790  \n",
       "TCGA-CS-4941       8.608326        8.344526  \n",
       "TCGA-CS-4942       9.279502        7.034985  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(511, 18328)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Hybridization REF</th>\n",
       "      <th>Composite Element REF</th>\n",
       "      <th>A1BG</th>\n",
       "      <th>A1CF</th>\n",
       "      <th>A2BP1</th>\n",
       "      <th>A2LD1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TCGA-CS-4938</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.683179</td>\n",
       "      <td>0.776869</td>\n",
       "      <td>0.652055</td>\n",
       "      <td>0.919739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-CS-4941</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.521934</td>\n",
       "      <td>0.784401</td>\n",
       "      <td>0.563447</td>\n",
       "      <td>0.865717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-CS-4942</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.610067</td>\n",
       "      <td>0.828194</td>\n",
       "      <td>0.607771</td>\n",
       "      <td>0.875369</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Hybridization REF  Composite Element REF      A1BG      A1CF     A2BP1  \\\n",
       "TCGA-CS-4938                         NaN  0.683179  0.776869  0.652055   \n",
       "TCGA-CS-4941                         NaN  0.521934  0.784401  0.563447   \n",
       "TCGA-CS-4942                         NaN  0.610067  0.828194  0.607771   \n",
       "\n",
       "Hybridization REF     A2LD1  \n",
       "TCGA-CS-4938       0.919739  \n",
       "TCGA-CS-4941       0.865717  \n",
       "TCGA-CS-4942       0.875369  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(511, 20115)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Hybridization REF</th>\n",
       "      <th>Composite Element REF</th>\n",
       "      <th>years_to_birth</th>\n",
       "      <th>vital_status</th>\n",
       "      <th>days_to_death</th>\n",
       "      <th>days_to_last_followup</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Patient_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TCGA-CS-4938</th>\n",
       "      <td>value</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-CS-4941</th>\n",
       "      <td>value</td>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>234</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-CS-4942</th>\n",
       "      <td>value</td>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "      <td>1335</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Hybridization REF Composite Element REF years_to_birth vital_status  \\\n",
       "Patient_ID                                                            \n",
       "TCGA-CS-4938                      value             31            0   \n",
       "TCGA-CS-4941                      value             67            1   \n",
       "TCGA-CS-4942                      value             44            1   \n",
       "\n",
       "Hybridization REF days_to_death days_to_last_followup  \n",
       "Patient_ID                                             \n",
       "TCGA-CS-4938                NaN                  3574  \n",
       "TCGA-CS-4941                234                   NaN  \n",
       "TCGA-CS-4942               1335                   NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(511, 14)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "histological_type\n",
       "astrocytoma          193\n",
       "oligodendroglioma    191\n",
       "oligoastrocytoma     127\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(mirna_processed.iloc[:3,:5])\n",
    "display(mirna_processed.shape)\n",
    "\n",
    "display(rna_processed.iloc[:3,:5])\n",
    "display(rna_processed.shape)\n",
    "\n",
    "display(meth_processed.iloc[:3,:5])\n",
    "display(meth_processed.shape)\n",
    "\n",
    "display(clinical_processed.iloc[:3,:5])\n",
    "display(clinical_processed.shape)\n",
    "\n",
    "display(targets.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc9e7f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-08 13:06:12,202 - bioneuralnet.utils.data - INFO - Starting Beta-to-M value conversion (shape: (511, 20114)). Epsilon: 1e-06\n",
      "2025-11-08 13:06:13,301 - bioneuralnet.utils.data - INFO - Beta-to-M conversion complete.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Methylation shape: (511, 20114)\n",
      "RNA shape: (511, 18328)\n",
      "miRNA shape: (511, 548)\n",
      "Clinical shape: (511, 13)\n",
      "target\n",
      "0         193\n",
      "1         191\n",
      "2         127\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import bioneuralnet as bnn\n",
    "\n",
    "# drop unwanted columns from clinical data\n",
    "clinical_processed.drop(columns=[\"Composite Element REF\"], errors=\"ignore\", inplace=True)\n",
    "\n",
    "# we transform the methylation beta values to M-values and drop unwanted columns\n",
    "meth_m = meth_processed.drop(columns=[\"Composite Element REF\"], errors=\"ignore\")\n",
    "\n",
    "# convert beta values to M-values using bioneuralnet utility with small epsilon to avoid log(0)\n",
    "meth_m = bnn.utils.beta_to_m(meth_m, eps=1e-6) \n",
    "\n",
    "# lastly we turn the target labels into numerical classes\n",
    "mapping = {\"astrocytoma\": 0, \"oligodendroglioma\": 1, \"oligoastrocytoma\": 2}\n",
    "target_labels = targets.map(mapping).to_frame(name=\"target\")\n",
    "\n",
    "# as a safety check we align the indices once more\n",
    "X_meth = meth_m.loc[common_patients]\n",
    "X_rna = rna_processed.loc[common_patients]\n",
    "X_mirna = mirna_processed.loc[common_patients]\n",
    "Y_labels = target_labels.loc[common_patients]\n",
    "clinical_final = clinical_processed.loc[common_patients]\n",
    "\n",
    "print(f\"\\nDNA_Methylation shape: {X_meth.shape}\")\n",
    "print(f\"RNA shape: {X_rna.shape}\")\n",
    "print(f\"miRNA shape: {X_mirna.shape}\")\n",
    "print(f\"Clinical shape: {clinical_final.shape}\")\n",
    "print(Y_labels.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0643ec13",
   "metadata": {},
   "source": [
    "## Feature Selection Methodology\n",
    "\n",
    "### Supported Methods and Interpretation\n",
    "\n",
    "**BioNeuralNet** provides three techniques for feature selection, allowing for different views of the data's statistical profile:\n",
    "\n",
    "- **Variance Thresholding:** Identifies features with the **highest overall variance** across all samples.\n",
    "\n",
    "- **ANOVA F-test:** Pinpoints features that best **distinguish between the target classes** (KIRC, KIRP, and KICH).\n",
    "\n",
    "- **Random Forest Importance:** Assesses **feature utility** based on its contribution to a predictive non-linear model.\n",
    "\n",
    "### GBMLGG Cohort Selection Strategy\n",
    "\n",
    "A dimensionality reduction step was essential for managing the high-feature-count omics data:\n",
    "\n",
    "- **High-Feature Datasets:** Both DNA Methylation (20,114) and RNA (18,328) required significant feature reduction.\n",
    "\n",
    "- **Filtering Process:** The **top 6,000 features** were initially extracted from the Methylation and RNA datasets using all three methods.\n",
    "\n",
    "- **Final Set:** A consensus set was built by finding the intersection of features selected by the ANOVA F-test and Random Forest Importance, ensuring both statistical relevance and model-based utility.\n",
    "\n",
    "- **Low-Feature Datasets:** The miRNA data (548 features) was passed through **without selection**, as its feature count was already manageable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "978e9281",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-08 13:15:33,012 - bioneuralnet.utils.preprocess - INFO - [Inf]: Replaced 0 infinite values\n",
      "2025-11-08 13:15:33,012 - bioneuralnet.utils.preprocess - INFO - [NaN]: Replaced 0 NaNs after median imputation\n",
      "2025-11-08 13:15:33,012 - bioneuralnet.utils.preprocess - INFO - [Zero-Var]: 0 columns dropped due to zero variance\n",
      "2025-11-08 13:15:33,085 - bioneuralnet.utils.preprocess - INFO - Selected top 6000 features by variance\n",
      "2025-11-08 13:15:35,778 - bioneuralnet.utils.preprocess - INFO - [Inf]: Replaced 0 infinite values\n",
      "2025-11-08 13:15:35,779 - bioneuralnet.utils.preprocess - INFO - [NaN]: Replaced 0 NaNs after median imputation\n",
      "2025-11-08 13:15:35,779 - bioneuralnet.utils.preprocess - INFO - [Zero-Var]: 0 columns dropped due to zero variance\n",
      "2025-11-08 13:15:35,843 - bioneuralnet.utils.preprocess - INFO - Selected top 6000 features by variance\n",
      "2025-11-08 13:15:38,824 - bioneuralnet.utils.preprocess - INFO - [Inf]: Replaced 0 infinite values\n",
      "2025-11-08 13:15:38,824 - bioneuralnet.utils.preprocess - INFO - [NaN]: Replaced 0 NaNs after median imputation\n",
      "2025-11-08 13:15:38,824 - bioneuralnet.utils.preprocess - INFO - [Zero-Var]: 0 columns dropped due to zero variance\n",
      "2025-11-08 13:15:38,962 - bioneuralnet.utils.preprocess - INFO - Selected 6000 features by ANOVA (task=classification), 12015 significant, 0 padded\n",
      "2025-11-08 13:15:41,666 - bioneuralnet.utils.preprocess - INFO - [Inf]: Replaced 0 infinite values\n",
      "2025-11-08 13:15:41,666 - bioneuralnet.utils.preprocess - INFO - [NaN]: Replaced 0 NaNs after median imputation\n",
      "2025-11-08 13:15:41,666 - bioneuralnet.utils.preprocess - INFO - [Zero-Var]: 0 columns dropped due to zero variance\n",
      "2025-11-08 13:15:41,773 - bioneuralnet.utils.preprocess - INFO - Selected 6000 features by ANOVA (task=classification), 10820 significant, 0 padded\n",
      "2025-11-08 13:15:44,759 - bioneuralnet.utils.preprocess - INFO - [Inf]: Replaced 0 infinite values\n",
      "2025-11-08 13:15:44,760 - bioneuralnet.utils.preprocess - INFO - [NaN]: Replaced 0 NaNs after median imputation\n",
      "2025-11-08 13:15:44,760 - bioneuralnet.utils.preprocess - INFO - [Zero-Var]: 0 columns dropped due to zero variance\n",
      "2025-11-08 13:15:49,982 - bioneuralnet.utils.preprocess - INFO - [Inf]: Replaced 0 infinite values\n",
      "2025-11-08 13:15:49,982 - bioneuralnet.utils.preprocess - INFO - [NaN]: Replaced 0 NaNs after median imputation\n",
      "2025-11-08 13:15:49,983 - bioneuralnet.utils.preprocess - INFO - [Zero-Var]: 0 columns dropped due to zero variance\n"
     ]
    }
   ],
   "source": [
    "import bioneuralnet as bnn\n",
    "\n",
    "# feature selection\n",
    "meth_highvar = bnn.utils.select_top_k_variance(X_meth, k=6000)\n",
    "rna_highvar = bnn.utils.select_top_k_variance(X_rna, k=6000)\n",
    "\n",
    "meth_af = bnn.utils.top_anova_f_features(X_meth, Y_labels, max_features=6000)\n",
    "rna_af = bnn.utils.top_anova_f_features(X_rna, Y_labels, max_features=6000)\n",
    "\n",
    "meth_rf = bnn.utils.select_top_randomforest(X_meth, Y_labels, top_k=6000)\n",
    "rna_rf = bnn.utils.select_top_randomforest(X_rna, Y_labels, top_k=6000)\n",
    "\n",
    "meth_var_set = set(meth_highvar.columns)\n",
    "meth_anova_set = set(meth_af.columns)\n",
    "meth_rf_set = set(meth_rf.columns)\n",
    "\n",
    "rna_var_set = set(rna_highvar.columns)\n",
    "rna_anova_set = set(rna_af.columns)\n",
    "rna_rf_set = set(rna_rf.columns)\n",
    "\n",
    "meth_inter1 = list(meth_anova_set & meth_var_set)\n",
    "meth_inter2 = list(meth_rf_set & meth_var_set)\n",
    "meth_inter3 = list(meth_anova_set & meth_rf_set)\n",
    "meth_all_three = list(meth_anova_set & meth_var_set & meth_rf_set)\n",
    "\n",
    "rna_inter4 = list(rna_anova_set & rna_var_set)\n",
    "rna_inter5 = list(rna_rf_set & rna_var_set)\n",
    "rna_inter6 = list(rna_anova_set & rna_rf_set)\n",
    "rna_all_three = list(rna_anova_set & rna_var_set & rna_rf_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "467d9ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FROM THE 6000 Methylation feature selection:\n",
      "\n",
      "Anova-F & variance selection share: 2704 features\n",
      "Random Forest & variance selection share: 1768 features\n",
      "Anova-F & Random Forest share: 1823 features\n",
      "All three methods agree on: 809 features\n"
     ]
    }
   ],
   "source": [
    "print(\"FROM THE 6000 Methylation feature selection:\\n\")\n",
    "print(f\"Anova-F & variance selection share: {len(meth_inter1)} features\")\n",
    "print(f\"Random Forest & variance selection share: {len(meth_inter2)} features\")\n",
    "print(f\"Anova-F & Random Forest share: {len(meth_inter3)} features\")\n",
    "print(f\"All three methods agree on: {len(meth_all_three)} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6fb1e10b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FROM THE 6000 RNA feature selection:\n",
      "\n",
      "Anova-F & variance selection share: 2183 features\n",
      "Random Forest & variance selection share: 1977 features\n",
      "Anova-F & Random Forest share: 2127 features\n",
      "All three methods agree on: 763 features\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nFROM THE 6000 RNA feature selection:\\n\")\n",
    "print(f\"Anova-F & variance selection share: {len(rna_inter4)} features\")\n",
    "print(f\"Random Forest & variance selection share: {len(rna_inter5)} features\")\n",
    "print(f\"Anova-F & Random Forest share: {len(rna_inter6)} features\")\n",
    "print(f\"All three methods agree on: {len(rna_all_three)} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2ce6f8",
   "metadata": {},
   "source": [
    "## Feature Selection Summary: ANOVA-RF Intersection\n",
    "\n",
    "The chosen strategy for feature selection is based on the **overlap** between features identified by the **ANOVA F-test** and **Random Forest Importance**. This approach offers comprehensive filtering by balancing class-based relevance (ANOVA) with non-linear model importance (Random Forest). The resulting feature sets are considered the most robust for downstream analysis.\n",
    "\n",
    "### Feature Overlap Results\n",
    "\n",
    "The following table details the number of features resulting from the intersection of different selection methods for each omics data type.\n",
    "\n",
    "| Omics Data Type | ANOVA-F & Variance | RF & Variance | ANOVA-F & Random Forest (Selected) | All Three Agree |\n",
    "| :--- | :--- | :--- | :--- | :--- |\n",
    "| **Methylation** | 2,704 features | 1,768 features | **1,823 features** | 809 features |\n",
    "| **RNA** | 2,183 features | 1,977 features | **2,127 features** | 763 features |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1934672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Shapes for Modeling\n",
      "Methylation (X1): (511, 1823)\n",
      "RNA-Seq (X2): (511, 2127)\n",
      "miRNA-Seq (X3): (511, 548)\n",
      "Labels (Y): (511, 1)\n"
     ]
    }
   ],
   "source": [
    "X_meth_selected = X_meth[meth_inter3]\n",
    "X_rna_selected = X_rna[rna_inter6]\n",
    "\n",
    "print(\"\\nFinal Shapes for Modeling\")\n",
    "print(f\"Methylation (X1): {X_meth_selected.shape}\")\n",
    "print(f\"RNA-Seq (X2): {X_rna_selected.shape}\")\n",
    "print(f\"miRNA-Seq (X3): {X_mirna.shape}\")\n",
    "print(f\"Labels (Y): {Y_labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9348ca2a",
   "metadata": {},
   "source": [
    "## Data Availability\n",
    "\n",
    "To facilitate rapid experimentation and reproduction of our results, the fully processed and feature-selected dataset used in this analysis has been made available directly within the package.\n",
    "\n",
    "Users can load this dataset, bypassing all preceding data acquisition, preprocessing, and feature selection steps. This allows users to proceed immediately from this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08ec1120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mirna': (511, 548),\n",
       " 'target': (511, 1),\n",
       " 'clinical': (511, 13),\n",
       " 'rna': (511, 2127),\n",
       " 'meth': (511, 1823)}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import bioneuralnet as bnn\n",
    "\n",
    "tgca_gbmlgg = bnn.datasets.DatasetLoader(\"gbmlgg\")\n",
    "display(tgca_gbmlgg.shape)\n",
    "\n",
    "# The dataset is returned as a dictionary. We extract each file independetly based on the name( Key).\n",
    "dna_meth = tgca_gbmlgg.data[\"meth\"]\n",
    "rna = tgca_gbmlgg.data[\"rna\"]\n",
    "mirna = tgca_gbmlgg.data[\"mirna\"]\n",
    "clinical = tgca_gbmlgg.data[\"clinical\"]\n",
    "target = tgca_gbmlgg.data[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35abee6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples dropped by dropna: 0\n",
      "Final shape of clinical data: (511, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>years_to_birth</th>\n",
       "      <th>vital_status</th>\n",
       "      <th>days_to_death</th>\n",
       "      <th>days_to_last_followup</th>\n",
       "      <th>tumor_tissue_site</th>\n",
       "      <th>gender</th>\n",
       "      <th>date_of_initial_pathologic_diagnosis</th>\n",
       "      <th>radiation_therapy</th>\n",
       "      <th>karnofsky_performance_score</th>\n",
       "      <th>race</th>\n",
       "      <th>ethnicity</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Patient_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TCGA-CS-4938</th>\n",
       "      <td>31.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3574.0</td>\n",
       "      <td>central nervous system</td>\n",
       "      <td>female</td>\n",
       "      <td>2005</td>\n",
       "      <td>no</td>\n",
       "      <td>90.0</td>\n",
       "      <td>white</td>\n",
       "      <td>not hispanic or latino</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-CS-4941</th>\n",
       "      <td>67.0</td>\n",
       "      <td>1</td>\n",
       "      <td>234.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>central nervous system</td>\n",
       "      <td>male</td>\n",
       "      <td>2005</td>\n",
       "      <td>yes</td>\n",
       "      <td>90.0</td>\n",
       "      <td>white</td>\n",
       "      <td>not hispanic or latino</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-CS-4942</th>\n",
       "      <td>44.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1335.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>central nervous system</td>\n",
       "      <td>female</td>\n",
       "      <td>2006</td>\n",
       "      <td>yes</td>\n",
       "      <td>70.0</td>\n",
       "      <td>black or african american</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-CS-4943</th>\n",
       "      <td>37.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1106.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>central nervous system</td>\n",
       "      <td>male</td>\n",
       "      <td>2009</td>\n",
       "      <td>no</td>\n",
       "      <td>50.0</td>\n",
       "      <td>white</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-CS-4944</th>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1828.0</td>\n",
       "      <td>central nervous system</td>\n",
       "      <td>male</td>\n",
       "      <td>2010</td>\n",
       "      <td>yes</td>\n",
       "      <td>100.0</td>\n",
       "      <td>white</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-WY-A85A</th>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1320.0</td>\n",
       "      <td>central nervous system</td>\n",
       "      <td>male</td>\n",
       "      <td>2010</td>\n",
       "      <td>no</td>\n",
       "      <td>NaN</td>\n",
       "      <td>white</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-WY-A85B</th>\n",
       "      <td>24.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1393.0</td>\n",
       "      <td>central nervous system</td>\n",
       "      <td>male</td>\n",
       "      <td>2010</td>\n",
       "      <td>no</td>\n",
       "      <td>NaN</td>\n",
       "      <td>white</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-WY-A85C</th>\n",
       "      <td>36.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1426.0</td>\n",
       "      <td>central nervous system</td>\n",
       "      <td>male</td>\n",
       "      <td>2010</td>\n",
       "      <td>yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>white</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-WY-A85D</th>\n",
       "      <td>60.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1147.0</td>\n",
       "      <td>central nervous system</td>\n",
       "      <td>male</td>\n",
       "      <td>2010</td>\n",
       "      <td>no</td>\n",
       "      <td>NaN</td>\n",
       "      <td>white</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-WY-A85E</th>\n",
       "      <td>48.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>633.0</td>\n",
       "      <td>central nervous system</td>\n",
       "      <td>female</td>\n",
       "      <td>2011</td>\n",
       "      <td>no</td>\n",
       "      <td>NaN</td>\n",
       "      <td>white</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>511 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              years_to_birth  vital_status  days_to_death  \\\n",
       "Patient_ID                                                  \n",
       "TCGA-CS-4938            31.0             0            NaN   \n",
       "TCGA-CS-4941            67.0             1          234.0   \n",
       "TCGA-CS-4942            44.0             1         1335.0   \n",
       "TCGA-CS-4943            37.0             1         1106.0   \n",
       "TCGA-CS-4944            50.0             0            NaN   \n",
       "...                      ...           ...            ...   \n",
       "TCGA-WY-A85A            20.0             0            NaN   \n",
       "TCGA-WY-A85B            24.0             0            NaN   \n",
       "TCGA-WY-A85C            36.0             0            NaN   \n",
       "TCGA-WY-A85D            60.0             0            NaN   \n",
       "TCGA-WY-A85E            48.0             0            NaN   \n",
       "\n",
       "              days_to_last_followup       tumor_tissue_site  gender  \\\n",
       "Patient_ID                                                            \n",
       "TCGA-CS-4938                 3574.0  central nervous system  female   \n",
       "TCGA-CS-4941                    NaN  central nervous system    male   \n",
       "TCGA-CS-4942                    NaN  central nervous system  female   \n",
       "TCGA-CS-4943                    NaN  central nervous system    male   \n",
       "TCGA-CS-4944                 1828.0  central nervous system    male   \n",
       "...                             ...                     ...     ...   \n",
       "TCGA-WY-A85A                 1320.0  central nervous system    male   \n",
       "TCGA-WY-A85B                 1393.0  central nervous system    male   \n",
       "TCGA-WY-A85C                 1426.0  central nervous system    male   \n",
       "TCGA-WY-A85D                 1147.0  central nervous system    male   \n",
       "TCGA-WY-A85E                  633.0  central nervous system  female   \n",
       "\n",
       "              date_of_initial_pathologic_diagnosis radiation_therapy  \\\n",
       "Patient_ID                                                             \n",
       "TCGA-CS-4938                                  2005                no   \n",
       "TCGA-CS-4941                                  2005               yes   \n",
       "TCGA-CS-4942                                  2006               yes   \n",
       "TCGA-CS-4943                                  2009                no   \n",
       "TCGA-CS-4944                                  2010               yes   \n",
       "...                                            ...               ...   \n",
       "TCGA-WY-A85A                                  2010                no   \n",
       "TCGA-WY-A85B                                  2010                no   \n",
       "TCGA-WY-A85C                                  2010               yes   \n",
       "TCGA-WY-A85D                                  2010                no   \n",
       "TCGA-WY-A85E                                  2011                no   \n",
       "\n",
       "              karnofsky_performance_score                       race  \\\n",
       "Patient_ID                                                             \n",
       "TCGA-CS-4938                         90.0                      white   \n",
       "TCGA-CS-4941                         90.0                      white   \n",
       "TCGA-CS-4942                         70.0  black or african american   \n",
       "TCGA-CS-4943                         50.0                      white   \n",
       "TCGA-CS-4944                        100.0                      white   \n",
       "...                                   ...                        ...   \n",
       "TCGA-WY-A85A                          NaN                      white   \n",
       "TCGA-WY-A85B                          NaN                      white   \n",
       "TCGA-WY-A85C                          NaN                      white   \n",
       "TCGA-WY-A85D                          NaN                      white   \n",
       "TCGA-WY-A85E                          NaN                      white   \n",
       "\n",
       "                           ethnicity  \n",
       "Patient_ID                            \n",
       "TCGA-CS-4938  not hispanic or latino  \n",
       "TCGA-CS-4941  not hispanic or latino  \n",
       "TCGA-CS-4942                     NaN  \n",
       "TCGA-CS-4943                     NaN  \n",
       "TCGA-CS-4944                     NaN  \n",
       "...                              ...  \n",
       "TCGA-WY-A85A                     NaN  \n",
       "TCGA-WY-A85B                     NaN  \n",
       "TCGA-WY-A85C                     NaN  \n",
       "TCGA-WY-A85D                     NaN  \n",
       "TCGA-WY-A85E                     NaN  \n",
       "\n",
       "[511 rows x 11 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clinical = tgca_gbmlgg.data[\"clinical\"]\n",
    "samples_before = clinical.shape[1]\n",
    "\n",
    "clinical_half_len = clinical.shape[1] // 2\n",
    "clinical.dropna(inplace=True, axis=1, thresh=clinical_half_len)\n",
    "samples_after = clinical.shape[1]\n",
    "print(f\"Samples dropped by dropna: {samples_before - samples_after}\")\n",
    "print(f\"Final shape of clinical data: {clinical.shape}\")\n",
    "clinical.drop(columns=['histological_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09c76e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-13 01:46:19,984 - bioneuralnet.utils.preprocess - INFO - [Inf]: Replaced 0 infinite values\n",
      "2025-11-13 01:46:19,984 - bioneuralnet.utils.preprocess - INFO - [NaN]: Replaced 206 NaNs after median imputation\n",
      "2025-11-13 01:46:19,984 - bioneuralnet.utils.preprocess - INFO - [Zero-Var]: 0 columns dropped due to zero variance\n",
      "2025-11-13 01:46:20,066 - bioneuralnet.utils.preprocess - INFO - Selected top 4 features by RandomForest importance\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>karnofsky_performance_score</th>\n",
       "      <th>gender_male</th>\n",
       "      <th>radiation_therapy_no</th>\n",
       "      <th>ethnicity_hispanic or latino</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Patient_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TCGA-CS-4938</th>\n",
       "      <td>90.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-CS-4941</th>\n",
       "      <td>90.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-CS-4942</th>\n",
       "      <td>70.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              karnofsky_performance_score  gender_male  radiation_therapy_no  \\\n",
       "Patient_ID                                                                     \n",
       "TCGA-CS-4938                         90.0        False                  True   \n",
       "TCGA-CS-4941                         90.0         True                 False   \n",
       "TCGA-CS-4942                         70.0        False                 False   \n",
       "\n",
       "              ethnicity_hispanic or latino  \n",
       "Patient_ID                                  \n",
       "TCGA-CS-4938                         False  \n",
       "TCGA-CS-4941                         False  \n",
       "TCGA-CS-4942                         False  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# BioNeuralNet provides a preprocessing function to handle clinical data\n",
    "clinical = tgca_gbmlgg.data[\"clinical\"]\n",
    "#clinical = bnn.utils.data.impute_omics_knn(clinical)\n",
    "# For more details on the preprocessing functions, see `bioneuralnet.utils.preprocess``\n",
    "clinical_preprocessed = bnn.utils.preprocess_clinical(\n",
    "    clinical, \n",
    "    target, \n",
    "    top_k=4, \n",
    "    scale=False, \n",
    "    ignore_columns=[ \"vital_status\",\"histological_type\",\"days_to_last_followup\",  \"years_to_birth\", \"days_to_death\", \"date_of_initial_pathologic_diagnosis\"])\n",
    "\n",
    "clinical_preprocessed.columns\n",
    "display(clinical_preprocessed.iloc[:3,:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb76a376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nan values in X_train_full: 0\n",
      "Nan value in X_train_full after dropping: 0\n",
      "X_train_full shape: (511, 4498)\n",
      "\n",
      "Network shape: (4498, 4498)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "X_train_full = pd.concat([dna_meth, rna, mirna], axis=1)\n",
    "\n",
    "print(f\"Nan values in X_train_full: {X_train_full.isna().sum().sum()}\")\n",
    "X_train_full = X_train_full.dropna()\n",
    "print(f\"Nan value in X_train_full after dropping: {X_train_full.isna().sum().sum()}\")\n",
    "\n",
    "print(f\"X_train_full shape: {X_train_full.shape}\")\n",
    "# building the graph using the similarity graph function with k=15\n",
    "A_train = bnn.utils.gen_similarity_graph(X_train_full, k=15)\n",
    "\n",
    "print(f\"\\nNetwork shape: {A_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "37cf0642",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-13 01:46:38,771\tINFO worker.py:1888 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m Warning: The actor ImplicitFunc is very large (40 MiB). Check that its definition is not implicitly capturing a large array or other object in scope. Tip: use ray.put() to put large objects in the Ray object store.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(tune_train_n pid=237589)\u001b[0m /home/vicente/Github/BioNeuralNet/.enviroment/lib/python3.12/site-packages/ray/train/_internal/session.py:772: RayDeprecationWarning: `ray.train.report` should be switched to `ray.tune.report` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context: https://github.com/ray-project/ray/issues/49454\n",
      "\u001b[36m(tune_train_n pid=237589)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(tune_train_n pid=237589)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/vicente/ray_results/tune_dp/T4650b_00000/checkpoint_000000)\n",
      "\u001b[36m(tune_train_n pid=237589)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/vicente/ray_results/tune_dp/T4650b_00000/checkpoint_000001)\n",
      "\u001b[36m(tune_train_n pid=237589)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/vicente/ray_results/tune_dp/T4650b_00000/checkpoint_000002)\n",
      "\u001b[36m(tune_train_n pid=237589)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/vicente/ray_results/tune_dp/T4650b_00000/checkpoint_000003)\n",
      "\u001b[36m(tune_train_n pid=237589)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/vicente/ray_results/tune_dp/T4650b_00000/checkpoint_000004)\n",
      "\u001b[36m(tune_train_n pid=237589)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/vicente/ray_results/tune_dp/T4650b_00000/checkpoint_000005)\n",
      "\u001b[36m(tune_train_n pid=237589)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/vicente/ray_results/tune_dp/T4650b_00000/checkpoint_000006)\n",
      "\u001b[36m(tune_train_n pid=237589)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/vicente/ray_results/tune_dp/T4650b_00000/checkpoint_000007)\n",
      "\u001b[36m(tune_train_n pid=237589)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/vicente/ray_results/tune_dp/T4650b_00000/checkpoint_000008)\n",
      "\u001b[36m(tune_train_n pid=237589)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/vicente/ray_results/tune_dp/T4650b_00000/checkpoint_000009)\n",
      "\u001b[36m(tune_train_n pid=237589)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/vicente/ray_results/tune_dp/T4650b_00000/checkpoint_000010)\n",
      "\u001b[36m(tune_train_n pid=237589)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/vicente/ray_results/tune_dp/T4650b_00000/checkpoint_000011)\n",
      "\u001b[36m(tune_train_n pid=237589)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/vicente/ray_results/tune_dp/T4650b_00000/checkpoint_000012)\n",
      "\u001b[36m(tune_train_n pid=237589)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/vicente/ray_results/tune_dp/T4650b_00000/checkpoint_000013)\n",
      "\u001b[36m(tune_train_n pid=237589)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/vicente/ray_results/tune_dp/T4650b_00000/checkpoint_000014)\n",
      "\u001b[36m(tune_train_n pid=237589)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/vicente/ray_results/tune_dp/T4650b_00000/checkpoint_000015)\n",
      "\u001b[36m(tune_train_n pid=237589)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/vicente/ray_results/tune_dp/T4650b_00000/checkpoint_000016)\n",
      "\u001b[36m(tune_train_n pid=237814)\u001b[0m /home/vicente/Github/BioNeuralNet/.enviroment/lib/python3.12/site-packages/ray/train/_internal/session.py:772: RayDeprecationWarning: `ray.train.report` should be switched to `ray.tune.report` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context: https://github.com/ray-project/ray/issues/49454\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=237814)\u001b[0m   _log_deprecation_warning(\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=237814)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/vicente/ray_results/tune_dp/T4650b_00002/checkpoint_000000)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=237972)\u001b[0m /home/vicente/Github/BioNeuralNet/.enviroment/lib/python3.12/site-packages/ray/train/_internal/session.py:772: RayDeprecationWarning: `ray.train.report` should be switched to `ray.tune.report` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context: https://github.com/ray-project/ray/issues/49454\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=237972)\u001b[0m   _log_deprecation_warning(\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=237972)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/vicente/ray_results/tune_dp/T4650b_00004/checkpoint_000000)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=238129)\u001b[0m /home/vicente/Github/BioNeuralNet/.enviroment/lib/python3.12/site-packages/ray/train/_internal/session.py:772: RayDeprecationWarning: `ray.train.report` should be switched to `ray.tune.report` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context: https://github.com/ray-project/ray/issues/49454\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=238129)\u001b[0m   _log_deprecation_warning(\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=238129)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/vicente/ray_results/tune_dp/T4650b_00006/checkpoint_000000)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=238290)\u001b[0m /home/vicente/Github/BioNeuralNet/.enviroment/lib/python3.12/site-packages/ray/train/_internal/session.py:772: RayDeprecationWarning: `ray.train.report` should be switched to `ray.tune.report` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context: https://github.com/ray-project/ray/issues/49454\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=238290)\u001b[0m   _log_deprecation_warning(\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=238290)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/vicente/ray_results/tune_dp/T4650b_00008/checkpoint_000001)\u001b[32m [repeated 4x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m Warning: The actor ImplicitFunc is very large (40 MiB). Check that its definition is not implicitly capturing a large array or other object in scope. Tip: use ray.put() to put large objects in the Ray object store.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(tune_train_n pid=238459)\u001b[0m /home/vicente/Github/BioNeuralNet/.enviroment/lib/python3.12/site-packages/ray/train/_internal/session.py:772: RayDeprecationWarning: `ray.train.report` should be switched to `ray.tune.report` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context: https://github.com/ray-project/ray/issues/49454\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=238459)\u001b[0m   _log_deprecation_warning(\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=238459)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/vicente/ray_results/tune_dp/T649bc_00000/checkpoint_000004)\u001b[32m [repeated 105x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=238547)\u001b[0m /home/vicente/Github/BioNeuralNet/.enviroment/lib/python3.12/site-packages/ray/train/_internal/session.py:772: RayDeprecationWarning: `ray.train.report` should be switched to `ray.tune.report` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context: https://github.com/ray-project/ray/issues/49454\n",
      "\u001b[36m(tune_train_n pid=238547)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(tune_train_n pid=238547)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/vicente/ray_results/tune_dp/T649bc_00001/checkpoint_000000)\u001b[32m [repeated 96x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=238625)\u001b[0m /home/vicente/Github/BioNeuralNet/.enviroment/lib/python3.12/site-packages/ray/train/_internal/session.py:772: RayDeprecationWarning: `ray.train.report` should be switched to `ray.tune.report` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context: https://github.com/ray-project/ray/issues/49454\n",
      "\u001b[36m(tune_train_n pid=238625)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(tune_train_n pid=238712)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/vicente/ray_results/tune_dp/T649bc_00003/checkpoint_000001)\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=238792)\u001b[0m /home/vicente/Github/BioNeuralNet/.enviroment/lib/python3.12/site-packages/ray/train/_internal/session.py:772: RayDeprecationWarning: `ray.train.report` should be switched to `ray.tune.report` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context: https://github.com/ray-project/ray/issues/49454\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=238792)\u001b[0m   _log_deprecation_warning(\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=238872)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/vicente/ray_results/tune_dp/T649bc_00005/checkpoint_000000)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=238950)\u001b[0m /home/vicente/Github/BioNeuralNet/.enviroment/lib/python3.12/site-packages/ray/train/_internal/session.py:772: RayDeprecationWarning: `ray.train.report` should be switched to `ray.tune.report` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context: https://github.com/ray-project/ray/issues/49454\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=238950)\u001b[0m   _log_deprecation_warning(\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=239028)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/vicente/ray_results/tune_dp/T649bc_00007/checkpoint_000001)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=239107)\u001b[0m /home/vicente/Github/BioNeuralNet/.enviroment/lib/python3.12/site-packages/ray/train/_internal/session.py:772: RayDeprecationWarning: `ray.train.report` should be switched to `ray.tune.report` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context: https://github.com/ray-project/ray/issues/49454\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=239107)\u001b[0m   _log_deprecation_warning(\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=239186)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/vicente/ray_results/tune_dp/T649bc_00009/checkpoint_000003)\u001b[32m [repeated 6x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m Warning: The actor ImplicitFunc is very large (40 MiB). Check that its definition is not implicitly capturing a large array or other object in scope. Tip: use ray.put() to put large objects in the Ray object store.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(tune_train_n pid=239280)\u001b[0m /home/vicente/Github/BioNeuralNet/.enviroment/lib/python3.12/site-packages/ray/train/_internal/session.py:772: RayDeprecationWarning: `ray.train.report` should be switched to `ray.tune.report` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context: https://github.com/ray-project/ray/issues/49454\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=239280)\u001b[0m   _log_deprecation_warning(\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=239280)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/vicente/ray_results/tune_dp/T8c07c_00000/checkpoint_000001)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=239443)\u001b[0m /home/vicente/Github/BioNeuralNet/.enviroment/lib/python3.12/site-packages/ray/train/_internal/session.py:772: RayDeprecationWarning: `ray.train.report` should be switched to `ray.tune.report` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context: https://github.com/ray-project/ray/issues/49454\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=239443)\u001b[0m   _log_deprecation_warning(\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=239443)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/vicente/ray_results/tune_dp/T8c07c_00002/checkpoint_000000)\u001b[32m [repeated 115x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=239600)\u001b[0m /home/vicente/Github/BioNeuralNet/.enviroment/lib/python3.12/site-packages/ray/train/_internal/session.py:772: RayDeprecationWarning: `ray.train.report` should be switched to `ray.tune.report` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context: https://github.com/ray-project/ray/issues/49454\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=239600)\u001b[0m   _log_deprecation_warning(\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=239600)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/vicente/ray_results/tune_dp/T8c07c_00004/checkpoint_000000)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=239762)\u001b[0m /home/vicente/Github/BioNeuralNet/.enviroment/lib/python3.12/site-packages/ray/train/_internal/session.py:772: RayDeprecationWarning: `ray.train.report` should be switched to `ray.tune.report` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context: https://github.com/ray-project/ray/issues/49454\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=239762)\u001b[0m   _log_deprecation_warning(\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=239762)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/vicente/ray_results/tune_dp/T8c07c_00006/checkpoint_000005)\u001b[32m [repeated 38x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=239919)\u001b[0m /home/vicente/Github/BioNeuralNet/.enviroment/lib/python3.12/site-packages/ray/train/_internal/session.py:772: RayDeprecationWarning: `ray.train.report` should be switched to `ray.tune.report` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context: https://github.com/ray-project/ray/issues/49454\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=239919)\u001b[0m   _log_deprecation_warning(\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=239919)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/vicente/ray_results/tune_dp/T8c07c_00008/checkpoint_000000)\u001b[32m [repeated 28x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m Warning: The actor ImplicitFunc is very large (40 MiB). Check that its definition is not implicitly capturing a large array or other object in scope. Tip: use ray.put() to put large objects in the Ray object store.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(tune_train_n pid=240086)\u001b[0m /home/vicente/Github/BioNeuralNet/.enviroment/lib/python3.12/site-packages/ray/train/_internal/session.py:772: RayDeprecationWarning: `ray.train.report` should be switched to `ray.tune.report` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context: https://github.com/ray-project/ray/issues/49454\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=240086)\u001b[0m   _log_deprecation_warning(\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=240086)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/vicente/ray_results/tune_dp/Ta7a57_00000/checkpoint_000002)\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=240169)\u001b[0m /home/vicente/Github/BioNeuralNet/.enviroment/lib/python3.12/site-packages/ray/train/_internal/session.py:772: RayDeprecationWarning: `ray.train.report` should be switched to `ray.tune.report` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context: https://github.com/ray-project/ray/issues/49454\n",
      "\u001b[36m(tune_train_n pid=240169)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(tune_train_n pid=240169)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/vicente/ray_results/tune_dp/Ta7a57_00001/checkpoint_000001)\u001b[32m [repeated 99x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=240247)\u001b[0m /home/vicente/Github/BioNeuralNet/.enviroment/lib/python3.12/site-packages/ray/train/_internal/session.py:772: RayDeprecationWarning: `ray.train.report` should be switched to `ray.tune.report` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context: https://github.com/ray-project/ray/issues/49454\n",
      "\u001b[36m(tune_train_n pid=240247)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(tune_train_n pid=240326)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/vicente/ray_results/tune_dp/Ta7a57_00003/checkpoint_000006)\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=240404)\u001b[0m /home/vicente/Github/BioNeuralNet/.enviroment/lib/python3.12/site-packages/ray/train/_internal/session.py:772: RayDeprecationWarning: `ray.train.report` should be switched to `ray.tune.report` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context: https://github.com/ray-project/ray/issues/49454\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=240404)\u001b[0m   _log_deprecation_warning(\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=240482)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/vicente/ray_results/tune_dp/Ta7a57_00005/checkpoint_000001)\u001b[32m [repeated 96x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=240567)\u001b[0m /home/vicente/Github/BioNeuralNet/.enviroment/lib/python3.12/site-packages/ray/train/_internal/session.py:772: RayDeprecationWarning: `ray.train.report` should be switched to `ray.tune.report` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context: https://github.com/ray-project/ray/issues/49454\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=240567)\u001b[0m   _log_deprecation_warning(\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=240645)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/vicente/ray_results/tune_dp/Ta7a57_00007/checkpoint_000000)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=240723)\u001b[0m /home/vicente/Github/BioNeuralNet/.enviroment/lib/python3.12/site-packages/ray/train/_internal/session.py:772: RayDeprecationWarning: `ray.train.report` should be switched to `ray.tune.report` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context: https://github.com/ray-project/ray/issues/49454\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=240723)\u001b[0m   _log_deprecation_warning(\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=240723)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/vicente/ray_results/tune_dp/Ta7a57_00008/checkpoint_000001)\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m Warning: The actor ImplicitFunc is very large (40 MiB). Check that its definition is not implicitly capturing a large array or other object in scope. Tip: use ray.put() to put large objects in the Ray object store.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(tune_train_n pid=240909)\u001b[0m /home/vicente/Github/BioNeuralNet/.enviroment/lib/python3.12/site-packages/ray/train/_internal/session.py:772: RayDeprecationWarning: `ray.train.report` should be switched to `ray.tune.report` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context: https://github.com/ray-project/ray/issues/49454\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=240909)\u001b[0m   _log_deprecation_warning(\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=240909)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/vicente/ray_results/tune_dp/T119a5_00000/checkpoint_000004)\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=241071)\u001b[0m /home/vicente/Github/BioNeuralNet/.enviroment/lib/python3.12/site-packages/ray/train/_internal/session.py:772: RayDeprecationWarning: `ray.train.report` should be switched to `ray.tune.report` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context: https://github.com/ray-project/ray/issues/49454\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=241071)\u001b[0m   _log_deprecation_warning(\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=241071)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/vicente/ray_results/tune_dp/T119a5_00002/checkpoint_000000)\u001b[32m [repeated 97x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=241228)\u001b[0m /home/vicente/Github/BioNeuralNet/.enviroment/lib/python3.12/site-packages/ray/train/_internal/session.py:772: RayDeprecationWarning: `ray.train.report` should be switched to `ray.tune.report` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context: https://github.com/ray-project/ray/issues/49454\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=241228)\u001b[0m   _log_deprecation_warning(\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=241228)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/vicente/ray_results/tune_dp/T119a5_00004/checkpoint_000000)\u001b[32m [repeated 18x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=241385)\u001b[0m /home/vicente/Github/BioNeuralNet/.enviroment/lib/python3.12/site-packages/ray/train/_internal/session.py:772: RayDeprecationWarning: `ray.train.report` should be switched to `ray.tune.report` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context: https://github.com/ray-project/ray/issues/49454\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=241385)\u001b[0m   _log_deprecation_warning(\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=241385)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/vicente/ray_results/tune_dp/T119a5_00006/checkpoint_000001)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=241544)\u001b[0m /home/vicente/Github/BioNeuralNet/.enviroment/lib/python3.12/site-packages/ray/train/_internal/session.py:772: RayDeprecationWarning: `ray.train.report` should be switched to `ray.tune.report` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context: https://github.com/ray-project/ray/issues/49454\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=241544)\u001b[0m   _log_deprecation_warning(\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=241544)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/vicente/ray_results/tune_dp/T119a5_00008/checkpoint_000000)\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m Warning: The actor ImplicitFunc is very large (40 MiB). Check that its definition is not implicitly capturing a large array or other object in scope. Tip: use ray.put() to put large objects in the Ray object store.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(tune_train_n pid=241714)\u001b[0m /home/vicente/Github/BioNeuralNet/.enviroment/lib/python3.12/site-packages/ray/train/_internal/session.py:772: RayDeprecationWarning: `ray.train.report` should be switched to `ray.tune.report` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context: https://github.com/ray-project/ray/issues/49454\n",
      "\u001b[36m(tune_train_n pid=241714)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(tune_train_n pid=241714)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/vicente/ray_results/tune_dp/T2d826_00000/checkpoint_000000)\n",
      "\u001b[36m(tune_train_n pid=241714)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/vicente/ray_results/tune_dp/T2d826_00000/checkpoint_000001)\n",
      "\u001b[36m(tune_train_n pid=241714)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/vicente/ray_results/tune_dp/T2d826_00000/checkpoint_000002)\n",
      "\u001b[36m(tune_train_n pid=241714)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/vicente/ray_results/tune_dp/T2d826_00000/checkpoint_000003)\n",
      "\u001b[36m(tune_train_n pid=241714)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/vicente/ray_results/tune_dp/T2d826_00000/checkpoint_000004)\n",
      "\u001b[36m(tune_train_n pid=241714)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/vicente/ray_results/tune_dp/T2d826_00000/checkpoint_000005)\n",
      "\u001b[36m(tune_train_n pid=241714)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/vicente/ray_results/tune_dp/T2d826_00000/checkpoint_000006)\n",
      "\u001b[36m(tune_train_n pid=241714)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/vicente/ray_results/tune_dp/T2d826_00000/checkpoint_000007)\n",
      "\u001b[36m(tune_train_n pid=241714)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/vicente/ray_results/tune_dp/T2d826_00000/checkpoint_000008)\n",
      "\u001b[36m(tune_train_n pid=241714)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/vicente/ray_results/tune_dp/T2d826_00000/checkpoint_000009)\n",
      "\u001b[36m(tune_train_n pid=241714)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/vicente/ray_results/tune_dp/T2d826_00000/checkpoint_000010)\n",
      "\u001b[36m(tune_train_n pid=241714)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/vicente/ray_results/tune_dp/T2d826_00000/checkpoint_000011)\n",
      "\u001b[36m(tune_train_n pid=241714)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/vicente/ray_results/tune_dp/T2d826_00000/checkpoint_000012)\n",
      "\u001b[36m(tune_train_n pid=241714)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/vicente/ray_results/tune_dp/T2d826_00000/checkpoint_000013)\n",
      "\u001b[36m(tune_train_n pid=241714)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/vicente/ray_results/tune_dp/T2d826_00000/checkpoint_000014)\n",
      "\u001b[36m(tune_train_n pid=241714)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/vicente/ray_results/tune_dp/T2d826_00000/checkpoint_000015)\n",
      "\u001b[36m(tune_train_n pid=241714)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/vicente/ray_results/tune_dp/T2d826_00000/checkpoint_000016)\n",
      "\u001b[36m(tune_train_n pid=241883)\u001b[0m /home/vicente/Github/BioNeuralNet/.enviroment/lib/python3.12/site-packages/ray/train/_internal/session.py:772: RayDeprecationWarning: `ray.train.report` should be switched to `ray.tune.report` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context: https://github.com/ray-project/ray/issues/49454\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=241883)\u001b[0m   _log_deprecation_warning(\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=241883)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/vicente/ray_results/tune_dp/T2d826_00002/checkpoint_000011)\u001b[32m [repeated 112x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=242042)\u001b[0m /home/vicente/Github/BioNeuralNet/.enviroment/lib/python3.12/site-packages/ray/train/_internal/session.py:772: RayDeprecationWarning: `ray.train.report` should be switched to `ray.tune.report` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context: https://github.com/ray-project/ray/issues/49454\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=242042)\u001b[0m   _log_deprecation_warning(\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=242042)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/vicente/ray_results/tune_dp/T2d826_00004/checkpoint_000001)\u001b[32m [repeated 54x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=242200)\u001b[0m /home/vicente/Github/BioNeuralNet/.enviroment/lib/python3.12/site-packages/ray/train/_internal/session.py:772: RayDeprecationWarning: `ray.train.report` should be switched to `ray.tune.report` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context: https://github.com/ray-project/ray/issues/49454\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=242200)\u001b[0m   _log_deprecation_warning(\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=242200)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/vicente/ray_results/tune_dp/T2d826_00006/checkpoint_000000)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=242356)\u001b[0m /home/vicente/Github/BioNeuralNet/.enviroment/lib/python3.12/site-packages/ray/train/_internal/session.py:772: RayDeprecationWarning: `ray.train.report` should be switched to `ray.tune.report` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context: https://github.com/ray-project/ray/issues/49454\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=242356)\u001b[0m   _log_deprecation_warning(\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=242356)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/vicente/ray_results/tune_dp/T2d826_00008/checkpoint_000001)\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m Warning: The actor ImplicitFunc is very large (40 MiB). Check that its definition is not implicitly capturing a large array or other object in scope. Tip: use ray.put() to put large objects in the Ray object store.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(tune_train_n pid=242791)\u001b[0m /home/vicente/Github/BioNeuralNet/.enviroment/lib/python3.12/site-packages/ray/train/_internal/session.py:772: RayDeprecationWarning: `ray.train.report` should be switched to `ray.tune.report` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context: https://github.com/ray-project/ray/issues/49454\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=242791)\u001b[0m   _log_deprecation_warning(\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=242791)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/vicente/ray_results/tune_dp/T97a2f_00000/checkpoint_000001)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=242952)\u001b[0m /home/vicente/Github/BioNeuralNet/.enviroment/lib/python3.12/site-packages/ray/train/_internal/session.py:772: RayDeprecationWarning: `ray.train.report` should be switched to `ray.tune.report` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context: https://github.com/ray-project/ray/issues/49454\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=242952)\u001b[0m   _log_deprecation_warning(\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=242952)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/vicente/ray_results/tune_dp/T97a2f_00002/checkpoint_000000)\u001b[32m [repeated 100x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=243110)\u001b[0m /home/vicente/Github/BioNeuralNet/.enviroment/lib/python3.12/site-packages/ray/train/_internal/session.py:772: RayDeprecationWarning: `ray.train.report` should be switched to `ray.tune.report` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context: https://github.com/ray-project/ray/issues/49454\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=243110)\u001b[0m   _log_deprecation_warning(\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=243110)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/vicente/ray_results/tune_dp/T97a2f_00004/checkpoint_000000)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=243271)\u001b[0m /home/vicente/Github/BioNeuralNet/.enviroment/lib/python3.12/site-packages/ray/train/_internal/session.py:772: RayDeprecationWarning: `ray.train.report` should be switched to `ray.tune.report` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context: https://github.com/ray-project/ray/issues/49454\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=243271)\u001b[0m   _log_deprecation_warning(\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=243271)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/vicente/ray_results/tune_dp/T97a2f_00006/checkpoint_000007)\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=243428)\u001b[0m /home/vicente/Github/BioNeuralNet/.enviroment/lib/python3.12/site-packages/ray/train/_internal/session.py:772: RayDeprecationWarning: `ray.train.report` should be switched to `ray.tune.report` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context: https://github.com/ray-project/ray/issues/49454\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=243428)\u001b[0m   _log_deprecation_warning(\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=243428)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/vicente/ray_results/tune_dp/T97a2f_00008/checkpoint_000001)\u001b[32m [repeated 4x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m Warning: The actor ImplicitFunc is very large (40 MiB). Check that its definition is not implicitly capturing a large array or other object in scope. Tip: use ray.put() to put large objects in the Ray object store.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(tune_train_n pid=243608)\u001b[0m /home/vicente/Github/BioNeuralNet/.enviroment/lib/python3.12/site-packages/ray/train/_internal/session.py:772: RayDeprecationWarning: `ray.train.report` should be switched to `ray.tune.report` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context: https://github.com/ray-project/ray/issues/49454\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=243608)\u001b[0m   _log_deprecation_warning(\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=243608)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/vicente/ray_results/tune_dp/Tc3f1a_00000/checkpoint_000007)\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=243691)\u001b[0m /home/vicente/Github/BioNeuralNet/.enviroment/lib/python3.12/site-packages/ray/train/_internal/session.py:772: RayDeprecationWarning: `ray.train.report` should be switched to `ray.tune.report` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context: https://github.com/ray-project/ray/issues/49454\n",
      "\u001b[36m(tune_train_n pid=243691)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(tune_train_n pid=243691)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/vicente/ray_results/tune_dp/Tc3f1a_00001/checkpoint_000031)\u001b[32m [repeated 124x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=243769)\u001b[0m /home/vicente/Github/BioNeuralNet/.enviroment/lib/python3.12/site-packages/ray/train/_internal/session.py:772: RayDeprecationWarning: `ray.train.report` should be switched to `ray.tune.report` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context: https://github.com/ray-project/ray/issues/49454\n",
      "\u001b[36m(tune_train_n pid=243769)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(tune_train_n pid=243848)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/vicente/ray_results/tune_dp/Tc3f1a_00003/checkpoint_000000)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=243927)\u001b[0m /home/vicente/Github/BioNeuralNet/.enviroment/lib/python3.12/site-packages/ray/train/_internal/session.py:772: RayDeprecationWarning: `ray.train.report` should be switched to `ray.tune.report` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context: https://github.com/ray-project/ray/issues/49454\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=243927)\u001b[0m   _log_deprecation_warning(\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=244006)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/vicente/ray_results/tune_dp/Tc3f1a_00005/checkpoint_000001)\u001b[32m [repeated 19x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=244088)\u001b[0m /home/vicente/Github/BioNeuralNet/.enviroment/lib/python3.12/site-packages/ray/train/_internal/session.py:772: RayDeprecationWarning: `ray.train.report` should be switched to `ray.tune.report` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context: https://github.com/ray-project/ray/issues/49454\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=244088)\u001b[0m   _log_deprecation_warning(\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=244177)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/vicente/ray_results/tune_dp/Tc3f1a_00007/checkpoint_000001)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=244285)\u001b[0m /home/vicente/Github/BioNeuralNet/.enviroment/lib/python3.12/site-packages/ray/train/_internal/session.py:772: RayDeprecationWarning: `ray.train.report` should be switched to `ray.tune.report` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context: https://github.com/ray-project/ray/issues/49454\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=244285)\u001b[0m   _log_deprecation_warning(\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=244363)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/vicente/ray_results/tune_dp/Tc3f1a_00009/checkpoint_000000)\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m Warning: The actor ImplicitFunc is very large (40 MiB). Check that its definition is not implicitly capturing a large array or other object in scope. Tip: use ray.put() to put large objects in the Ray object store.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(tune_train_n pid=245041)\u001b[0m /home/vicente/Github/BioNeuralNet/.enviroment/lib/python3.12/site-packages/ray/train/_internal/session.py:772: RayDeprecationWarning: `ray.train.report` should be switched to `ray.tune.report` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context: https://github.com/ray-project/ray/issues/49454\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=245041)\u001b[0m   _log_deprecation_warning(\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=245041)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/vicente/ray_results/tune_dp/Te11cc_00000/checkpoint_000000)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=245138)\u001b[0m /home/vicente/Github/BioNeuralNet/.enviroment/lib/python3.12/site-packages/ray/train/_internal/session.py:772: RayDeprecationWarning: `ray.train.report` should be switched to `ray.tune.report` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context: https://github.com/ray-project/ray/issues/49454\n",
      "\u001b[36m(tune_train_n pid=245138)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(tune_train_n pid=245138)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/vicente/ray_results/tune_dp/Te11cc_00001/checkpoint_000001)\u001b[32m [repeated 101x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=245218)\u001b[0m /home/vicente/Github/BioNeuralNet/.enviroment/lib/python3.12/site-packages/ray/train/_internal/session.py:772: RayDeprecationWarning: `ray.train.report` should be switched to `ray.tune.report` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context: https://github.com/ray-project/ray/issues/49454\n",
      "\u001b[36m(tune_train_n pid=245218)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(tune_train_n pid=245298)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/vicente/ray_results/tune_dp/Te11cc_00003/checkpoint_000000)\u001b[32m [repeated 18x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=245376)\u001b[0m /home/vicente/Github/BioNeuralNet/.enviroment/lib/python3.12/site-packages/ray/train/_internal/session.py:772: RayDeprecationWarning: `ray.train.report` should be switched to `ray.tune.report` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context: https://github.com/ray-project/ray/issues/49454\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=245376)\u001b[0m   _log_deprecation_warning(\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=245454)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/vicente/ray_results/tune_dp/Te11cc_00005/checkpoint_000000)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=245533)\u001b[0m /home/vicente/Github/BioNeuralNet/.enviroment/lib/python3.12/site-packages/ray/train/_internal/session.py:772: RayDeprecationWarning: `ray.train.report` should be switched to `ray.tune.report` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context: https://github.com/ray-project/ray/issues/49454\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=245533)\u001b[0m   _log_deprecation_warning(\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=245611)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/vicente/ray_results/tune_dp/Te11cc_00007/checkpoint_000001)\u001b[32m [repeated 18x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=245690)\u001b[0m /home/vicente/Github/BioNeuralNet/.enviroment/lib/python3.12/site-packages/ray/train/_internal/session.py:772: RayDeprecationWarning: `ray.train.report` should be switched to `ray.tune.report` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context: https://github.com/ray-project/ray/issues/49454\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=245690)\u001b[0m   _log_deprecation_warning(\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(tune_train_n pid=245769)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/vicente/ray_results/tune_dp/Te11cc_00009/checkpoint_000002)\u001b[32m [repeated 4x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m Warning: The actor ImplicitFunc is very large (40 MiB). Check that its definition is not implicitly capturing a large array or other object in scope. Tip: use ray.put() to put large objects in the Ray object store.\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import random\n",
    "# import logging \n",
    "# import warnings \n",
    "# import numpy as np\n",
    "# import torch\n",
    "# import ray\n",
    "# import bioneuralnet as bnn\n",
    "# from bioneuralnet.utils import logger \n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "\n",
    "# if not ray.is_initialized():\n",
    "\n",
    "#     # 3. Set the ray init logging level to INFO\n",
    "#     ray.init(logging_level=logging.INFO) \n",
    "    \n",
    "#     # Ignore common warnings\n",
    "#     warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "#     warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a2b14e",
   "metadata": {},
   "source": [
    "## Reproducibility and Seeding\n",
    "\n",
    "To ensure our experimental results are fully reproducible, a single global seed is set at the beginning of the analysis.\n",
    "\n",
    "This utility function propagates the seed to all sources of randomness, including `random`, `numpy`, and `torch` (for both CPU and GPU). Critically, it also configures the PyTorch cuDNN backend to use deterministic algorithms.\n",
    "\n",
    "**for each DPMON outer iteration, the seed is incremented to generate a differnt internal test/train split.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3f28d80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-13 01:46:39,215 - bioneuralnet.utils.data - INFO - Setting global seed for reproducibility to: 118\n",
      "2025-11-13 01:46:39,216 - bioneuralnet.utils.data - INFO - CUDA available. Applying seed to all GPU operations\n",
      "2025-11-13 01:46:39,217 - bioneuralnet.utils.data - INFO - Seed setting complete\n"
     ]
    }
   ],
   "source": [
    "import bioneuralnet as bnn\n",
    "\n",
    "SEED = 118\n",
    "bnn.utils.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44593d0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88084af0",
   "metadata": {},
   "source": [
    "## Classification using DPMON: Training and Evaluation\n",
    "\n",
    "* Run 3 outer iterations, each with a different seed.\n",
    "* Each iteration performs hyperparameter tuning.\n",
    "* After tuning, train `repeat_num = 3` models with the best hyperparameters.\n",
    "* Collect predictions from the best model of each iteration.\n",
    "* Compute **Accuracy**, **F1 Weighted**, and **F1 Macro +/- standard deviation** across iterations.\n",
    "\n",
    "This demonstrates the **end-to-end BioNeuralNet pipeline** in action.\n",
    "\n",
    "### Analysis of Hyperparameter Optimization\n",
    "\n",
    "The hyperparameter tuning results below showcase the best configuration found across three distinct GNN model runs.\n",
    "\n",
    "| Parameter | SAGE (GraphSAGE) | GCN (Graph Convolutional) | GAT (Graph Attention) |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **gnn_layer_num** | 8 | 4 | 8 |\n",
    "| **gnn_hidden_dim** | 32 | 16 | 32 |\n",
    "| **lr (Learning Rate)** | 0.000401 | 0.001687 | 0.000401 |\n",
    "| **weight_decay** | 0.007823 | 0.000269 | 0.007823 |\n",
    "| **nn_hidden_dim1** | 32 | 64 | 32 |\n",
    "| **nn_hidden_dim2** | 32 | 64 | 32 |\n",
    "| **num_epochs** | 2048 | 2048 | 2048 |\n",
    "\n",
    "\n",
    "### Results\n",
    "\n",
    "| Metric | SAGE | GCN | GAT |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **Accuracy** | 0.9472 +/- 0.0747 | 0.9954 +/- 0.0065 | 0.9993 +/- 0.0009 |\n",
    "| **F1 Weighted** | 0.9464 +/- 0.0758 | 0.9954 +/- 0.0064 | 0.9993 +/- 0.0009 |\n",
    "| **F1 Macro** | 0.9510 +/- 0.0693 | 0.9957 +/- 0.0061 | 0.9993 +/- 0.0010 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a77d244",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-13 01:46:39,241 - bioneuralnet.utils.data - INFO - Setting global seed for reproducibility to: 118\n",
      "2025-11-13 01:46:39,242 - bioneuralnet.utils.data - INFO - CUDA available. Applying seed to all GPU operations\n",
      "2025-11-13 01:46:39,242 - bioneuralnet.utils.data - INFO - Seed setting complete\n",
      "2025-11-13 01:46:39,243 - bioneuralnet.downstream_task.dpmon - INFO - Output directory set to: /home/vicente/Github/BioNeuralNet/dpmon_results_SAGE_FINAL\n",
      "2025-11-13 01:46:39,243 - bioneuralnet.downstream_task.dpmon - INFO - Initialized DPMON with the provided parameters.\n",
      "2025-11-13 01:46:39,243 - bioneuralnet.downstream_task.dpmon - INFO - Starting DPMON run.\n",
      "2025-11-13 01:46:39,264 - bioneuralnet.downstream_task.dpmon - INFO - Running hyperparameter tuning for DPMON.\n",
      "2025-11-13 01:46:39,265 - bioneuralnet.downstream_task.dpmon - INFO - Using GPU 0\n",
      "2025-11-13 01:46:39,442 - bioneuralnet.downstream_task.dpmon - INFO - Number of nodes in network: 4498\n",
      "2025-11-13 01:46:41,154 - bioneuralnet.downstream_task.dpmon - INFO - Starting hyperparameter tuning for dataset shape: (511, 4499)\n",
      "2025-11-13 01:46:41,155\tINFO tune.py:616 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n",
      "2025-11-13 01:47:15,869\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/vicente/ray_results/tune_dp' in 0.0038s.\n",
      "2025-11-13 01:47:15,896 - bioneuralnet.downstream_task.dpmon - INFO - Best trial config: {'gnn_layer_num': 4, 'gnn_hidden_dim': 16, 'lr': 0.0016874617558272796, 'weight_decay': 0.0002694172269687386, 'nn_hidden_dim1': 64, 'nn_hidden_dim2': 64, 'num_epochs': 2048}\n",
      "2025-11-13 01:47:15,896 - bioneuralnet.downstream_task.dpmon - INFO - Best trial final loss: 0.5534491539001465\n",
      "2025-11-13 01:47:15,897 - bioneuralnet.downstream_task.dpmon - INFO - Best trial final accuracy: 1.0\n",
      "2025-11-13 01:47:15,900 - bioneuralnet.downstream_task.dpmon - INFO -    gnn_layer_num  gnn_hidden_dim        lr  weight_decay  nn_hidden_dim1  \\\n",
      "0              4              16  0.001687      0.000269              64   \n",
      "\n",
      "   nn_hidden_dim2  num_epochs  \n",
      "0              64        2048  \n",
      "2025-11-13 01:47:15,901 - bioneuralnet.downstream_task.dpmon - INFO - Best tuned parameters: {'gnn_layer_num': 4, 'gnn_hidden_dim': 16, 'lr': 0.0016874617558272796, 'weight_decay': 0.0002694172269687386, 'nn_hidden_dim1': 64, 'nn_hidden_dim2': 64, 'num_epochs': 2048}\n",
      "2025-11-13 01:47:15,902 - bioneuralnet.downstream_task.dpmon - INFO - Best tuned parameters: {'gnn_layer_num': 4, 'gnn_hidden_dim': 16, 'lr': 0.0016874617558272796, 'weight_decay': 0.0002694172269687386, 'nn_hidden_dim1': 64, 'nn_hidden_dim2': 64, 'num_epochs': 2048}\n",
      "2025-11-13 01:47:15,902 - bioneuralnet.downstream_task.dpmon - INFO - Running standard training with tuned parameters.\n",
      "2025-11-13 01:47:15,902 - bioneuralnet.downstream_task.dpmon - INFO - Using GPU 0\n",
      "2025-11-13 01:47:16,083 - bioneuralnet.downstream_task.dpmon - INFO - Number of nodes in network: 4498\n",
      "2025-11-13 01:47:17,951 - bioneuralnet.downstream_task.dpmon - INFO - Training iteration 1/3\n",
      "2025-11-13 01:47:21,931 - bioneuralnet.downstream_task.dpmon - INFO - Training Accuracy: 1.0000\n",
      "2025-11-13 01:47:21,943 - bioneuralnet.downstream_task.dpmon - INFO - Model saved to /home/vicente/Github/BioNeuralNet/dpmon_results_SAGE_FINAL/dpm_model_iter_1.pth\n",
      "2025-11-13 01:47:21,945 - bioneuralnet.downstream_task.dpmon - INFO - Training iteration 2/3\n",
      "2025-11-13 01:47:26,109 - bioneuralnet.downstream_task.dpmon - INFO - Training Accuracy: 0.8082\n",
      "2025-11-13 01:47:26,122 - bioneuralnet.downstream_task.dpmon - INFO - Model saved to /home/vicente/Github/BioNeuralNet/dpmon_results_SAGE_FINAL/dpm_model_iter_2.pth\n",
      "2025-11-13 01:47:26,125 - bioneuralnet.downstream_task.dpmon - INFO - Training iteration 3/3\n",
      "2025-11-13 01:47:30,054 - bioneuralnet.downstream_task.dpmon - INFO - Training Accuracy: 1.0000\n",
      "2025-11-13 01:47:30,067 - bioneuralnet.downstream_task.dpmon - INFO - Model saved to /home/vicente/Github/BioNeuralNet/dpmon_results_SAGE_FINAL/dpm_model_iter_3.pth\n",
      "2025-11-13 01:47:30,068 - bioneuralnet.downstream_task.dpmon - INFO - Best Accuracy: 1.0000\n",
      "2025-11-13 01:47:30,069 - bioneuralnet.downstream_task.dpmon - INFO - Average Accuracy across 3 models: 0.9361\n",
      "2025-11-13 01:47:30,069 - bioneuralnet.downstream_task.dpmon - INFO - Standard Deviation across all models: 0.1107\n",
      "2025-11-13 01:47:30,069 - bioneuralnet.downstream_task.dpmon - INFO - Returning best model predictions and average accuracy (predictions, avg_accuracy).\n",
      "2025-11-13 01:47:30,071 - bioneuralnet.utils.data - INFO - Setting global seed for reproducibility to: 119\n",
      "2025-11-13 01:47:30,071 - bioneuralnet.utils.data - INFO - CUDA available. Applying seed to all GPU operations\n",
      "2025-11-13 01:47:30,071 - bioneuralnet.utils.data - INFO - Seed setting complete\n",
      "2025-11-13 01:47:30,072 - bioneuralnet.downstream_task.dpmon - INFO - Output directory set to: /home/vicente/Github/BioNeuralNet/dpmon_results_SAGE_FINAL\n",
      "2025-11-13 01:47:30,072 - bioneuralnet.downstream_task.dpmon - INFO - Initialized DPMON with the provided parameters.\n",
      "2025-11-13 01:47:30,072 - bioneuralnet.downstream_task.dpmon - INFO - Starting DPMON run.\n",
      "2025-11-13 01:47:30,083 - bioneuralnet.downstream_task.dpmon - INFO - Running hyperparameter tuning for DPMON.\n",
      "2025-11-13 01:47:30,084 - bioneuralnet.downstream_task.dpmon - INFO - Using GPU 0\n",
      "2025-11-13 01:47:30,254 - bioneuralnet.downstream_task.dpmon - INFO - Number of nodes in network: 4498\n",
      "2025-11-13 01:47:31,983 - bioneuralnet.downstream_task.dpmon - INFO - Starting hyperparameter tuning for dataset shape: (511, 4499)\n",
      "2025-11-13 01:47:31,983\tINFO tune.py:616 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n",
      "2025-11-13 01:48:09,051\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/vicente/ray_results/tune_dp' in 0.0026s.\n",
      "2025-11-13 01:48:09,074 - bioneuralnet.downstream_task.dpmon - INFO - Best trial config: {'gnn_layer_num': 8, 'gnn_hidden_dim': 16, 'lr': 0.0031152551982754385, 'weight_decay': 0.0004093955830993196, 'nn_hidden_dim1': 64, 'nn_hidden_dim2': 64, 'num_epochs': 4096}\n",
      "2025-11-13 01:48:09,074 - bioneuralnet.downstream_task.dpmon - INFO - Best trial final loss: 0.5521202087402344\n",
      "2025-11-13 01:48:09,074 - bioneuralnet.downstream_task.dpmon - INFO - Best trial final accuracy: 1.0\n",
      "2025-11-13 01:48:09,077 - bioneuralnet.downstream_task.dpmon - INFO -    gnn_layer_num  gnn_hidden_dim        lr  weight_decay  nn_hidden_dim1  \\\n",
      "0              8              16  0.003115      0.000409              64   \n",
      "\n",
      "   nn_hidden_dim2  num_epochs  \n",
      "0              64        4096  \n",
      "2025-11-13 01:48:09,080 - bioneuralnet.downstream_task.dpmon - INFO - Best tuned parameters: {'gnn_layer_num': 8, 'gnn_hidden_dim': 16, 'lr': 0.0031152551982754385, 'weight_decay': 0.0004093955830993196, 'nn_hidden_dim1': 64, 'nn_hidden_dim2': 64, 'num_epochs': 4096}\n",
      "2025-11-13 01:48:09,080 - bioneuralnet.downstream_task.dpmon - INFO - Best tuned parameters: {'gnn_layer_num': 8, 'gnn_hidden_dim': 16, 'lr': 0.0031152551982754385, 'weight_decay': 0.0004093955830993196, 'nn_hidden_dim1': 64, 'nn_hidden_dim2': 64, 'num_epochs': 4096}\n",
      "2025-11-13 01:48:09,081 - bioneuralnet.downstream_task.dpmon - INFO - Running standard training with tuned parameters.\n",
      "2025-11-13 01:48:09,081 - bioneuralnet.downstream_task.dpmon - INFO - Using GPU 0\n",
      "2025-11-13 01:48:09,269 - bioneuralnet.downstream_task.dpmon - INFO - Number of nodes in network: 4498\n",
      "2025-11-13 01:48:11,164 - bioneuralnet.downstream_task.dpmon - INFO - Training iteration 1/3\n",
      "2025-11-13 01:48:19,452 - bioneuralnet.downstream_task.dpmon - INFO - Training Accuracy: 1.0000\n",
      "2025-11-13 01:48:19,464 - bioneuralnet.downstream_task.dpmon - INFO - Model saved to /home/vicente/Github/BioNeuralNet/dpmon_results_SAGE_FINAL/dpm_model_iter_1.pth\n",
      "2025-11-13 01:48:19,466 - bioneuralnet.downstream_task.dpmon - INFO - Training iteration 2/3\n",
      "2025-11-13 01:48:27,621 - bioneuralnet.downstream_task.dpmon - INFO - Training Accuracy: 1.0000\n",
      "2025-11-13 01:48:27,624 - bioneuralnet.downstream_task.dpmon - INFO - Model saved to /home/vicente/Github/BioNeuralNet/dpmon_results_SAGE_FINAL/dpm_model_iter_2.pth\n",
      "2025-11-13 01:48:27,625 - bioneuralnet.downstream_task.dpmon - INFO - Training iteration 3/3\n",
      "2025-11-13 01:48:36,159 - bioneuralnet.downstream_task.dpmon - INFO - Training Accuracy: 0.6419\n",
      "2025-11-13 01:48:36,162 - bioneuralnet.downstream_task.dpmon - INFO - Model saved to /home/vicente/Github/BioNeuralNet/dpmon_results_SAGE_FINAL/dpm_model_iter_3.pth\n",
      "2025-11-13 01:48:36,164 - bioneuralnet.downstream_task.dpmon - INFO - Best Accuracy: 1.0000\n",
      "2025-11-13 01:48:36,164 - bioneuralnet.downstream_task.dpmon - INFO - Average Accuracy across 3 models: 0.8806\n",
      "2025-11-13 01:48:36,164 - bioneuralnet.downstream_task.dpmon - INFO - Standard Deviation across all models: 0.2068\n",
      "2025-11-13 01:48:36,164 - bioneuralnet.downstream_task.dpmon - INFO - Returning best model predictions and average accuracy (predictions, avg_accuracy).\n",
      "2025-11-13 01:48:36,167 - bioneuralnet.utils.data - INFO - Setting global seed for reproducibility to: 120\n",
      "2025-11-13 01:48:36,168 - bioneuralnet.utils.data - INFO - CUDA available. Applying seed to all GPU operations\n",
      "2025-11-13 01:48:36,168 - bioneuralnet.utils.data - INFO - Seed setting complete\n",
      "2025-11-13 01:48:36,168 - bioneuralnet.downstream_task.dpmon - INFO - Output directory set to: /home/vicente/Github/BioNeuralNet/dpmon_results_SAGE_FINAL\n",
      "2025-11-13 01:48:36,169 - bioneuralnet.downstream_task.dpmon - INFO - Initialized DPMON with the provided parameters.\n",
      "2025-11-13 01:48:36,169 - bioneuralnet.downstream_task.dpmon - INFO - Starting DPMON run.\n",
      "2025-11-13 01:48:36,180 - bioneuralnet.downstream_task.dpmon - INFO - Running hyperparameter tuning for DPMON.\n",
      "2025-11-13 01:48:36,180 - bioneuralnet.downstream_task.dpmon - INFO - Using GPU 0\n",
      "2025-11-13 01:48:36,352 - bioneuralnet.downstream_task.dpmon - INFO - Number of nodes in network: 4498\n",
      "2025-11-13 01:48:38,129 - bioneuralnet.downstream_task.dpmon - INFO - Starting hyperparameter tuning for dataset shape: (511, 4499)\n",
      "2025-11-13 01:48:38,129\tINFO tune.py:616 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n",
      "2025-11-13 01:49:17,222\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/vicente/ray_results/tune_dp' in 0.0117s.\n",
      "2025-11-13 01:49:17,257 - bioneuralnet.downstream_task.dpmon - INFO - Best trial config: {'gnn_layer_num': 2, 'gnn_hidden_dim': 16, 'lr': 0.0027003956214294257, 'weight_decay': 0.0024031420469003027, 'nn_hidden_dim1': 64, 'nn_hidden_dim2': 8, 'num_epochs': 512}\n",
      "2025-11-13 01:49:17,257 - bioneuralnet.downstream_task.dpmon - INFO - Best trial final loss: 0.5989490747451782\n",
      "2025-11-13 01:49:17,257 - bioneuralnet.downstream_task.dpmon - INFO - Best trial final accuracy: 1.0\n",
      "2025-11-13 01:49:17,260 - bioneuralnet.downstream_task.dpmon - INFO -    gnn_layer_num  gnn_hidden_dim      lr  weight_decay  nn_hidden_dim1  \\\n",
      "0              2              16  0.0027      0.002403              64   \n",
      "\n",
      "   nn_hidden_dim2  num_epochs  \n",
      "0               8         512  \n",
      "2025-11-13 01:49:17,262 - bioneuralnet.downstream_task.dpmon - INFO - Best tuned parameters: {'gnn_layer_num': 2, 'gnn_hidden_dim': 16, 'lr': 0.0027003956214294257, 'weight_decay': 0.0024031420469003027, 'nn_hidden_dim1': 64, 'nn_hidden_dim2': 8, 'num_epochs': 512}\n",
      "2025-11-13 01:49:17,262 - bioneuralnet.downstream_task.dpmon - INFO - Best tuned parameters: {'gnn_layer_num': 2, 'gnn_hidden_dim': 16, 'lr': 0.0027003956214294257, 'weight_decay': 0.0024031420469003027, 'nn_hidden_dim1': 64, 'nn_hidden_dim2': 8, 'num_epochs': 512}\n",
      "2025-11-13 01:49:17,262 - bioneuralnet.downstream_task.dpmon - INFO - Running standard training with tuned parameters.\n",
      "2025-11-13 01:49:17,263 - bioneuralnet.downstream_task.dpmon - INFO - Using GPU 0\n",
      "2025-11-13 01:49:17,439 - bioneuralnet.downstream_task.dpmon - INFO - Number of nodes in network: 4498\n",
      "2025-11-13 01:49:19,244 - bioneuralnet.downstream_task.dpmon - INFO - Training iteration 1/3\n",
      "2025-11-13 01:49:20,319 - bioneuralnet.downstream_task.dpmon - INFO - Training Accuracy: 0.9961\n",
      "2025-11-13 01:49:20,324 - bioneuralnet.downstream_task.dpmon - INFO - Model saved to /home/vicente/Github/BioNeuralNet/dpmon_results_SAGE_FINAL/dpm_model_iter_1.pth\n",
      "2025-11-13 01:49:20,328 - bioneuralnet.downstream_task.dpmon - INFO - Training iteration 2/3\n",
      "2025-11-13 01:49:21,466 - bioneuralnet.downstream_task.dpmon - INFO - Training Accuracy: 1.0000\n",
      "2025-11-13 01:49:21,468 - bioneuralnet.downstream_task.dpmon - INFO - Model saved to /home/vicente/Github/BioNeuralNet/dpmon_results_SAGE_FINAL/dpm_model_iter_2.pth\n",
      "2025-11-13 01:49:21,470 - bioneuralnet.downstream_task.dpmon - INFO - Training iteration 3/3\n",
      "2025-11-13 01:49:22,388 - bioneuralnet.downstream_task.dpmon - INFO - Training Accuracy: 0.9961\n",
      "2025-11-13 01:49:22,391 - bioneuralnet.downstream_task.dpmon - INFO - Model saved to /home/vicente/Github/BioNeuralNet/dpmon_results_SAGE_FINAL/dpm_model_iter_3.pth\n",
      "2025-11-13 01:49:22,392 - bioneuralnet.downstream_task.dpmon - INFO - Best Accuracy: 1.0000\n",
      "2025-11-13 01:49:22,393 - bioneuralnet.downstream_task.dpmon - INFO - Average Accuracy across 3 models: 0.9974\n",
      "2025-11-13 01:49:22,393 - bioneuralnet.downstream_task.dpmon - INFO - Standard Deviation across all models: 0.0023\n",
      "2025-11-13 01:49:22,393 - bioneuralnet.downstream_task.dpmon - INFO - Returning best model predictions and average accuracy (predictions, avg_accuracy).\n",
      "2025-11-13 01:49:22,395 - bioneuralnet.utils.data - INFO - Setting global seed for reproducibility to: 121\n",
      "2025-11-13 01:49:22,396 - bioneuralnet.utils.data - INFO - CUDA available. Applying seed to all GPU operations\n",
      "2025-11-13 01:49:22,396 - bioneuralnet.utils.data - INFO - Seed setting complete\n",
      "2025-11-13 01:49:22,396 - bioneuralnet.downstream_task.dpmon - INFO - Output directory set to: /home/vicente/Github/BioNeuralNet/dpmon_results_SAGE_FINAL\n",
      "2025-11-13 01:49:22,396 - bioneuralnet.downstream_task.dpmon - INFO - Initialized DPMON with the provided parameters.\n",
      "2025-11-13 01:49:22,397 - bioneuralnet.downstream_task.dpmon - INFO - Starting DPMON run.\n",
      "2025-11-13 01:49:22,410 - bioneuralnet.downstream_task.dpmon - INFO - Running hyperparameter tuning for DPMON.\n",
      "2025-11-13 01:49:22,410 - bioneuralnet.downstream_task.dpmon - INFO - Using GPU 0\n",
      "2025-11-13 01:49:22,756 - bioneuralnet.downstream_task.dpmon - INFO - Number of nodes in network: 4498\n",
      "2025-11-13 01:49:24,449 - bioneuralnet.downstream_task.dpmon - INFO - Starting hyperparameter tuning for dataset shape: (511, 4499)\n",
      "2025-11-13 01:49:24,450\tINFO tune.py:616 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n",
      "2025-11-13 01:50:00,161\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/vicente/ray_results/tune_dp' in 0.0029s.\n",
      "2025-11-13 01:50:00,190 - bioneuralnet.downstream_task.dpmon - INFO - Best trial config: {'gnn_layer_num': 8, 'gnn_hidden_dim': 128, 'lr': 0.00042884734295576757, 'weight_decay': 0.0004999049799078241, 'nn_hidden_dim1': 64, 'nn_hidden_dim2': 32, 'num_epochs': 8192}\n",
      "2025-11-13 01:50:00,191 - bioneuralnet.downstream_task.dpmon - INFO - Best trial final loss: 0.6153044104576111\n",
      "2025-11-13 01:50:00,191 - bioneuralnet.downstream_task.dpmon - INFO - Best trial final accuracy: 1.0\n",
      "2025-11-13 01:50:00,194 - bioneuralnet.downstream_task.dpmon - INFO -    gnn_layer_num  gnn_hidden_dim        lr  weight_decay  nn_hidden_dim1  \\\n",
      "0              8             128  0.000429        0.0005              64   \n",
      "\n",
      "   nn_hidden_dim2  num_epochs  \n",
      "0              32        8192  \n",
      "2025-11-13 01:50:00,195 - bioneuralnet.downstream_task.dpmon - INFO - Best tuned parameters: {'gnn_layer_num': 8, 'gnn_hidden_dim': 128, 'lr': 0.00042884734295576757, 'weight_decay': 0.0004999049799078241, 'nn_hidden_dim1': 64, 'nn_hidden_dim2': 32, 'num_epochs': 8192}\n",
      "2025-11-13 01:50:00,195 - bioneuralnet.downstream_task.dpmon - INFO - Best tuned parameters: {'gnn_layer_num': 8, 'gnn_hidden_dim': 128, 'lr': 0.00042884734295576757, 'weight_decay': 0.0004999049799078241, 'nn_hidden_dim1': 64, 'nn_hidden_dim2': 32, 'num_epochs': 8192}\n",
      "2025-11-13 01:50:00,196 - bioneuralnet.downstream_task.dpmon - INFO - Running standard training with tuned parameters.\n",
      "2025-11-13 01:50:00,196 - bioneuralnet.downstream_task.dpmon - INFO - Using GPU 0\n",
      "2025-11-13 01:50:00,372 - bioneuralnet.downstream_task.dpmon - INFO - Number of nodes in network: 4498\n",
      "2025-11-13 01:50:02,109 - bioneuralnet.downstream_task.dpmon - INFO - Training iteration 1/3\n",
      "2025-11-13 01:50:48,110 - bioneuralnet.downstream_task.dpmon - INFO - Training Accuracy: 0.9941\n",
      "2025-11-13 01:50:48,116 - bioneuralnet.downstream_task.dpmon - INFO - Model saved to /home/vicente/Github/BioNeuralNet/dpmon_results_SAGE_FINAL/dpm_model_iter_1.pth\n",
      "2025-11-13 01:50:48,121 - bioneuralnet.downstream_task.dpmon - INFO - Training iteration 2/3\n",
      "2025-11-13 01:51:34,168 - bioneuralnet.downstream_task.dpmon - INFO - Training Accuracy: 1.0000\n",
      "2025-11-13 01:51:34,171 - bioneuralnet.downstream_task.dpmon - INFO - Model saved to /home/vicente/Github/BioNeuralNet/dpmon_results_SAGE_FINAL/dpm_model_iter_2.pth\n",
      "2025-11-13 01:51:34,174 - bioneuralnet.downstream_task.dpmon - INFO - Training iteration 3/3\n",
      "2025-11-13 01:52:20,130 - bioneuralnet.downstream_task.dpmon - INFO - Training Accuracy: 1.0000\n",
      "2025-11-13 01:52:20,133 - bioneuralnet.downstream_task.dpmon - INFO - Model saved to /home/vicente/Github/BioNeuralNet/dpmon_results_SAGE_FINAL/dpm_model_iter_3.pth\n",
      "2025-11-13 01:52:20,136 - bioneuralnet.downstream_task.dpmon - INFO - Best Accuracy: 1.0000\n",
      "2025-11-13 01:52:20,136 - bioneuralnet.downstream_task.dpmon - INFO - Average Accuracy across 3 models: 0.9980\n",
      "2025-11-13 01:52:20,136 - bioneuralnet.downstream_task.dpmon - INFO - Standard Deviation across all models: 0.0034\n",
      "2025-11-13 01:52:20,137 - bioneuralnet.downstream_task.dpmon - INFO - Returning best model predictions and average accuracy (predictions, avg_accuracy).\n",
      "2025-11-13 01:52:20,139 - bioneuralnet.utils.data - INFO - Setting global seed for reproducibility to: 122\n",
      "2025-11-13 01:52:20,140 - bioneuralnet.utils.data - INFO - CUDA available. Applying seed to all GPU operations\n",
      "2025-11-13 01:52:20,140 - bioneuralnet.utils.data - INFO - Seed setting complete\n",
      "2025-11-13 01:52:20,140 - bioneuralnet.downstream_task.dpmon - INFO - Output directory set to: /home/vicente/Github/BioNeuralNet/dpmon_results_SAGE_FINAL\n",
      "2025-11-13 01:52:20,140 - bioneuralnet.downstream_task.dpmon - INFO - Initialized DPMON with the provided parameters.\n",
      "2025-11-13 01:52:20,140 - bioneuralnet.downstream_task.dpmon - INFO - Starting DPMON run.\n",
      "2025-11-13 01:52:20,151 - bioneuralnet.downstream_task.dpmon - INFO - Running hyperparameter tuning for DPMON.\n",
      "2025-11-13 01:52:20,152 - bioneuralnet.downstream_task.dpmon - INFO - Using GPU 0\n",
      "2025-11-13 01:52:20,474 - bioneuralnet.downstream_task.dpmon - INFO - Number of nodes in network: 4498\n",
      "2025-11-13 01:52:22,211 - bioneuralnet.downstream_task.dpmon - INFO - Starting hyperparameter tuning for dataset shape: (511, 4499)\n",
      "2025-11-13 01:52:22,211\tINFO tune.py:616 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n",
      "2025-11-13 01:53:02,442\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/vicente/ray_results/tune_dp' in 0.0040s.\n",
      "2025-11-13 01:53:02,473 - bioneuralnet.downstream_task.dpmon - INFO - Best trial config: {'gnn_layer_num': 8, 'gnn_hidden_dim': 16, 'lr': 0.0027948507420865174, 'weight_decay': 0.03230252176732325, 'nn_hidden_dim1': 64, 'nn_hidden_dim2': 32, 'num_epochs': 256}\n",
      "2025-11-13 01:53:02,473 - bioneuralnet.downstream_task.dpmon - INFO - Best trial final loss: 0.5776295065879822\n",
      "2025-11-13 01:53:02,473 - bioneuralnet.downstream_task.dpmon - INFO - Best trial final accuracy: 1.0\n",
      "2025-11-13 01:53:02,478 - bioneuralnet.downstream_task.dpmon - INFO -    gnn_layer_num  gnn_hidden_dim        lr  weight_decay  nn_hidden_dim1  \\\n",
      "0              8              16  0.002795      0.032303              64   \n",
      "\n",
      "   nn_hidden_dim2  num_epochs  \n",
      "0              32         256  \n",
      "2025-11-13 01:53:02,480 - bioneuralnet.downstream_task.dpmon - INFO - Best tuned parameters: {'gnn_layer_num': 8, 'gnn_hidden_dim': 16, 'lr': 0.0027948507420865174, 'weight_decay': 0.03230252176732325, 'nn_hidden_dim1': 64, 'nn_hidden_dim2': 32, 'num_epochs': 256}\n",
      "2025-11-13 01:53:02,480 - bioneuralnet.downstream_task.dpmon - INFO - Best tuned parameters: {'gnn_layer_num': 8, 'gnn_hidden_dim': 16, 'lr': 0.0027948507420865174, 'weight_decay': 0.03230252176732325, 'nn_hidden_dim1': 64, 'nn_hidden_dim2': 32, 'num_epochs': 256}\n",
      "2025-11-13 01:53:02,481 - bioneuralnet.downstream_task.dpmon - INFO - Running standard training with tuned parameters.\n",
      "2025-11-13 01:53:02,481 - bioneuralnet.downstream_task.dpmon - INFO - Using GPU 0\n",
      "2025-11-13 01:53:02,704 - bioneuralnet.downstream_task.dpmon - INFO - Number of nodes in network: 4498\n",
      "2025-11-13 01:53:04,622 - bioneuralnet.downstream_task.dpmon - INFO - Training iteration 1/3\n",
      "2025-11-13 01:53:05,335 - bioneuralnet.downstream_task.dpmon - INFO - Training Accuracy: 0.6164\n",
      "2025-11-13 01:53:05,338 - bioneuralnet.downstream_task.dpmon - INFO - Model saved to /home/vicente/Github/BioNeuralNet/dpmon_results_SAGE_FINAL/dpm_model_iter_1.pth\n",
      "2025-11-13 01:53:05,341 - bioneuralnet.downstream_task.dpmon - INFO - Training iteration 2/3\n",
      "2025-11-13 01:53:06,015 - bioneuralnet.downstream_task.dpmon - INFO - Training Accuracy: 0.3914\n",
      "2025-11-13 01:53:06,020 - bioneuralnet.downstream_task.dpmon - INFO - Model saved to /home/vicente/Github/BioNeuralNet/dpmon_results_SAGE_FINAL/dpm_model_iter_2.pth\n",
      "2025-11-13 01:53:06,023 - bioneuralnet.downstream_task.dpmon - INFO - Training iteration 3/3\n",
      "2025-11-13 01:53:06,817 - bioneuralnet.downstream_task.dpmon - INFO - Training Accuracy: 0.5127\n",
      "2025-11-13 01:53:06,820 - bioneuralnet.downstream_task.dpmon - INFO - Model saved to /home/vicente/Github/BioNeuralNet/dpmon_results_SAGE_FINAL/dpm_model_iter_3.pth\n",
      "2025-11-13 01:53:06,822 - bioneuralnet.downstream_task.dpmon - INFO - Best Accuracy: 0.6164\n",
      "2025-11-13 01:53:06,822 - bioneuralnet.downstream_task.dpmon - INFO - Average Accuracy across 3 models: 0.5068\n",
      "2025-11-13 01:53:06,822 - bioneuralnet.downstream_task.dpmon - INFO - Standard Deviation across all models: 0.1126\n",
      "2025-11-13 01:53:06,822 - bioneuralnet.downstream_task.dpmon - INFO - Returning best model predictions and average accuracy (predictions, avg_accuracy).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9233 +/- 0.1534\n",
      "F1 Weighted: 0.9023 +/- 0.1953\n",
      "F1 Macro: 0.9035 +/- 0.1931\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from bioneuralnet.downstream_task import DPMON\n",
    "\n",
    "output_dir_base_sage = Path(\"/home/vicente/Github/BioNeuralNet/dpmon_results_SAGE_FINAL\")\n",
    "target = target.rename(columns={\"target\": \"phenotype\"})\n",
    "\n",
    "n_repeats = 5\n",
    "all_preds = []\n",
    "\n",
    "for r in range(n_repeats):\n",
    "    bnn.utils.set_seed(SEED+r)\n",
    "    dpmon_repeat = DPMON(\n",
    "        adjacency_matrix=A_train,\n",
    "        omics_list=[dna_meth, rna, mirna],\n",
    "        phenotype_data=target,\n",
    "        clinical_data=clinical_preprocessed,\n",
    "        repeat_num=3,\n",
    "        model='SAGE',\n",
    "        tune=True,\n",
    "        gpu=True,\n",
    "        cuda=0,\n",
    "        output_dir=output_dir_base_sage,\n",
    "    )\n",
    "    \n",
    "    predictions_df, _ = dpmon_repeat.run()\n",
    "    all_preds.append(predictions_df[\"Predicted\"].values)\n",
    "\n",
    "all_preds = np.array(all_preds)\n",
    "\n",
    "f1_macro_list = [f1_score(target, pred, average='macro') for pred in all_preds]\n",
    "f1_weighted_list = [f1_score(target, pred, average='weighted') for pred in all_preds]\n",
    "accuracy_list = [accuracy_score(target, pred) for pred in all_preds]\n",
    "\n",
    "avg_f1_macro = np.mean(f1_macro_list)\n",
    "std_f1_macro = np.std(f1_macro_list)\n",
    "\n",
    "avg_f1_weighted = np.mean(f1_weighted_list)\n",
    "std_f1_weighted = np.std(f1_weighted_list)\n",
    "\n",
    "avg_acc = np.mean(accuracy_list)\n",
    "std_acc = np.std(accuracy_list)\n",
    "\n",
    "print(f\"Accuracy: {avg_acc:.4f} +/- {std_acc:.4f}\")\n",
    "print(f\"F1 Weighted: {avg_f1_weighted:.4f} +/- {std_f1_weighted:.4f}\")\n",
    "print(f\"F1 Macro: {avg_f1_macro:.4f} +/- {std_f1_macro:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b344bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-13 01:53:06,854 - bioneuralnet.utils.data - INFO - Setting global seed for reproducibility to: 118\n",
      "2025-11-13 01:53:06,855 - bioneuralnet.utils.data - INFO - CUDA available. Applying seed to all GPU operations\n",
      "2025-11-13 01:53:06,855 - bioneuralnet.utils.data - INFO - Seed setting complete\n",
      "2025-11-13 01:53:06,856 - bioneuralnet.downstream_task.dpmon - INFO - Output directory set to: /home/vicente/Github/BioNeuralNet/dpmon_results_GCN_FINAL\n",
      "2025-11-13 01:53:06,856 - bioneuralnet.downstream_task.dpmon - INFO - Initialized DPMON with the provided parameters.\n",
      "2025-11-13 01:53:06,856 - bioneuralnet.downstream_task.dpmon - INFO - Starting DPMON run.\n",
      "2025-11-13 01:53:06,870 - bioneuralnet.downstream_task.dpmon - INFO - Running hyperparameter tuning for DPMON.\n",
      "2025-11-13 01:53:06,871 - bioneuralnet.downstream_task.dpmon - INFO - Using GPU 0\n",
      "2025-11-13 01:53:07,213 - bioneuralnet.downstream_task.dpmon - INFO - Number of nodes in network: 4498\n",
      "2025-11-13 01:53:09,034 - bioneuralnet.downstream_task.dpmon - INFO - Starting hyperparameter tuning for dataset shape: (511, 4499)\n",
      "2025-11-13 01:53:09,034\tINFO tune.py:616 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n",
      "2025-11-13 01:53:44,069\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/vicente/ray_results/tune_dp' in 0.0029s.\n",
      "2025-11-13 01:53:44,099 - bioneuralnet.downstream_task.dpmon - INFO - Best trial config: {'gnn_layer_num': 2, 'gnn_hidden_dim': 128, 'lr': 0.005197038211089667, 'weight_decay': 0.04607928341516355, 'nn_hidden_dim1': 16, 'nn_hidden_dim2': 32, 'num_epochs': 4096}\n",
      "2025-11-13 01:53:44,100 - bioneuralnet.downstream_task.dpmon - INFO - Best trial final loss: 0.6415553689002991\n",
      "2025-11-13 01:53:44,100 - bioneuralnet.downstream_task.dpmon - INFO - Best trial final accuracy: 0.9706457925636007\n",
      "2025-11-13 01:53:44,104 - bioneuralnet.downstream_task.dpmon - INFO -    gnn_layer_num  gnn_hidden_dim        lr  weight_decay  nn_hidden_dim1  \\\n",
      "0              2             128  0.005197      0.046079              16   \n",
      "\n",
      "   nn_hidden_dim2  num_epochs  \n",
      "0              32        4096  \n",
      "2025-11-13 01:53:44,106 - bioneuralnet.downstream_task.dpmon - INFO - Best tuned parameters: {'gnn_layer_num': 2, 'gnn_hidden_dim': 128, 'lr': 0.005197038211089667, 'weight_decay': 0.04607928341516355, 'nn_hidden_dim1': 16, 'nn_hidden_dim2': 32, 'num_epochs': 4096}\n",
      "2025-11-13 01:53:44,106 - bioneuralnet.downstream_task.dpmon - INFO - Best tuned parameters: {'gnn_layer_num': 2, 'gnn_hidden_dim': 128, 'lr': 0.005197038211089667, 'weight_decay': 0.04607928341516355, 'nn_hidden_dim1': 16, 'nn_hidden_dim2': 32, 'num_epochs': 4096}\n",
      "2025-11-13 01:53:44,106 - bioneuralnet.downstream_task.dpmon - INFO - Running standard training with tuned parameters.\n",
      "2025-11-13 01:53:44,106 - bioneuralnet.downstream_task.dpmon - INFO - Using GPU 0\n",
      "2025-11-13 01:53:44,277 - bioneuralnet.downstream_task.dpmon - INFO - Number of nodes in network: 4498\n",
      "2025-11-13 01:53:45,985 - bioneuralnet.downstream_task.dpmon - INFO - Training iteration 1/3\n",
      "2025-11-13 01:54:32,311 - bioneuralnet.downstream_task.dpmon - INFO - Training Accuracy: 0.3738\n",
      "2025-11-13 01:54:32,313 - bioneuralnet.downstream_task.dpmon - INFO - Model saved to /home/vicente/Github/BioNeuralNet/dpmon_results_GCN_FINAL/dpm_model_iter_1.pth\n",
      "2025-11-13 01:54:32,319 - bioneuralnet.downstream_task.dpmon - INFO - Training iteration 2/3\n",
      "2025-11-13 01:55:18,608 - bioneuralnet.downstream_task.dpmon - INFO - Training Accuracy: 0.3757\n",
      "2025-11-13 01:55:18,610 - bioneuralnet.downstream_task.dpmon - INFO - Model saved to /home/vicente/Github/BioNeuralNet/dpmon_results_GCN_FINAL/dpm_model_iter_2.pth\n",
      "2025-11-13 01:55:18,617 - bioneuralnet.downstream_task.dpmon - INFO - Training iteration 3/3\n",
      "2025-11-13 01:56:04,984 - bioneuralnet.downstream_task.dpmon - INFO - Training Accuracy: 0.4168\n",
      "2025-11-13 01:56:04,986 - bioneuralnet.downstream_task.dpmon - INFO - Model saved to /home/vicente/Github/BioNeuralNet/dpmon_results_GCN_FINAL/dpm_model_iter_3.pth\n",
      "2025-11-13 01:56:04,993 - bioneuralnet.downstream_task.dpmon - INFO - Best Accuracy: 0.4168\n",
      "2025-11-13 01:56:04,993 - bioneuralnet.downstream_task.dpmon - INFO - Average Accuracy across 3 models: 0.3888\n",
      "2025-11-13 01:56:04,993 - bioneuralnet.downstream_task.dpmon - INFO - Standard Deviation across all models: 0.0243\n",
      "2025-11-13 01:56:04,994 - bioneuralnet.downstream_task.dpmon - INFO - Returning best model predictions and average accuracy (predictions, avg_accuracy).\n",
      "2025-11-13 01:56:04,997 - bioneuralnet.utils.data - INFO - Setting global seed for reproducibility to: 119\n",
      "2025-11-13 01:56:04,998 - bioneuralnet.utils.data - INFO - CUDA available. Applying seed to all GPU operations\n",
      "2025-11-13 01:56:04,998 - bioneuralnet.utils.data - INFO - Seed setting complete\n",
      "2025-11-13 01:56:04,999 - bioneuralnet.downstream_task.dpmon - INFO - Output directory set to: /home/vicente/Github/BioNeuralNet/dpmon_results_GCN_FINAL\n",
      "2025-11-13 01:56:04,999 - bioneuralnet.downstream_task.dpmon - INFO - Initialized DPMON with the provided parameters.\n",
      "2025-11-13 01:56:04,999 - bioneuralnet.downstream_task.dpmon - INFO - Starting DPMON run.\n",
      "2025-11-13 01:56:05,010 - bioneuralnet.downstream_task.dpmon - INFO - Running hyperparameter tuning for DPMON.\n",
      "2025-11-13 01:56:05,010 - bioneuralnet.downstream_task.dpmon - INFO - Using GPU 0\n",
      "2025-11-13 01:56:05,199 - bioneuralnet.downstream_task.dpmon - INFO - Number of nodes in network: 4498\n",
      "2025-11-13 01:56:07,079 - bioneuralnet.downstream_task.dpmon - INFO - Starting hyperparameter tuning for dataset shape: (511, 4499)\n",
      "2025-11-13 01:56:07,079\tINFO tune.py:616 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n",
      "2025-11-13 01:56:46,191\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/vicente/ray_results/tune_dp' in 0.0050s.\n",
      "2025-11-13 01:56:46,218 - bioneuralnet.downstream_task.dpmon - INFO - Best trial config: {'gnn_layer_num': 8, 'gnn_hidden_dim': 16, 'lr': 0.0031152551982754385, 'weight_decay': 0.0004093955830993196, 'nn_hidden_dim1': 64, 'nn_hidden_dim2': 64, 'num_epochs': 4096}\n",
      "2025-11-13 01:56:46,219 - bioneuralnet.downstream_task.dpmon - INFO - Best trial final loss: 0.5521262884140015\n",
      "2025-11-13 01:56:46,219 - bioneuralnet.downstream_task.dpmon - INFO - Best trial final accuracy: 1.0\n",
      "2025-11-13 01:56:46,222 - bioneuralnet.downstream_task.dpmon - INFO -    gnn_layer_num  gnn_hidden_dim        lr  weight_decay  nn_hidden_dim1  \\\n",
      "0              8              16  0.003115      0.000409              64   \n",
      "\n",
      "   nn_hidden_dim2  num_epochs  \n",
      "0              64        4096  \n",
      "2025-11-13 01:56:46,224 - bioneuralnet.downstream_task.dpmon - INFO - Best tuned parameters: {'gnn_layer_num': 8, 'gnn_hidden_dim': 16, 'lr': 0.0031152551982754385, 'weight_decay': 0.0004093955830993196, 'nn_hidden_dim1': 64, 'nn_hidden_dim2': 64, 'num_epochs': 4096}\n",
      "2025-11-13 01:56:46,224 - bioneuralnet.downstream_task.dpmon - INFO - Best tuned parameters: {'gnn_layer_num': 8, 'gnn_hidden_dim': 16, 'lr': 0.0031152551982754385, 'weight_decay': 0.0004093955830993196, 'nn_hidden_dim1': 64, 'nn_hidden_dim2': 64, 'num_epochs': 4096}\n",
      "2025-11-13 01:56:46,225 - bioneuralnet.downstream_task.dpmon - INFO - Running standard training with tuned parameters.\n",
      "2025-11-13 01:56:46,225 - bioneuralnet.downstream_task.dpmon - INFO - Using GPU 0\n",
      "2025-11-13 01:56:46,422 - bioneuralnet.downstream_task.dpmon - INFO - Number of nodes in network: 4498\n",
      "2025-11-13 01:56:48,201 - bioneuralnet.downstream_task.dpmon - INFO - Training iteration 1/3\n",
      "2025-11-13 01:56:58,751 - bioneuralnet.downstream_task.dpmon - INFO - Training Accuracy: 0.8728\n",
      "2025-11-13 01:56:58,753 - bioneuralnet.downstream_task.dpmon - INFO - Model saved to /home/vicente/Github/BioNeuralNet/dpmon_results_GCN_FINAL/dpm_model_iter_1.pth\n",
      "2025-11-13 01:56:58,755 - bioneuralnet.downstream_task.dpmon - INFO - Training iteration 2/3\n",
      "2025-11-13 01:57:09,053 - bioneuralnet.downstream_task.dpmon - INFO - Training Accuracy: 0.9530\n",
      "2025-11-13 01:57:09,056 - bioneuralnet.downstream_task.dpmon - INFO - Model saved to /home/vicente/Github/BioNeuralNet/dpmon_results_GCN_FINAL/dpm_model_iter_2.pth\n",
      "2025-11-13 01:57:09,058 - bioneuralnet.downstream_task.dpmon - INFO - Training iteration 3/3\n",
      "2025-11-13 01:57:19,252 - bioneuralnet.downstream_task.dpmon - INFO - Training Accuracy: 0.8493\n",
      "2025-11-13 01:57:19,254 - bioneuralnet.downstream_task.dpmon - INFO - Model saved to /home/vicente/Github/BioNeuralNet/dpmon_results_GCN_FINAL/dpm_model_iter_3.pth\n",
      "2025-11-13 01:57:19,256 - bioneuralnet.downstream_task.dpmon - INFO - Best Accuracy: 0.9530\n",
      "2025-11-13 01:57:19,257 - bioneuralnet.downstream_task.dpmon - INFO - Average Accuracy across 3 models: 0.8917\n",
      "2025-11-13 01:57:19,257 - bioneuralnet.downstream_task.dpmon - INFO - Standard Deviation across all models: 0.0544\n",
      "2025-11-13 01:57:19,257 - bioneuralnet.downstream_task.dpmon - INFO - Returning best model predictions and average accuracy (predictions, avg_accuracy).\n",
      "2025-11-13 01:57:19,260 - bioneuralnet.utils.data - INFO - Setting global seed for reproducibility to: 120\n",
      "2025-11-13 01:57:19,261 - bioneuralnet.utils.data - INFO - CUDA available. Applying seed to all GPU operations\n",
      "2025-11-13 01:57:19,261 - bioneuralnet.utils.data - INFO - Seed setting complete\n",
      "2025-11-13 01:57:19,261 - bioneuralnet.downstream_task.dpmon - INFO - Output directory set to: /home/vicente/Github/BioNeuralNet/dpmon_results_GCN_FINAL\n",
      "2025-11-13 01:57:19,261 - bioneuralnet.downstream_task.dpmon - INFO - Initialized DPMON with the provided parameters.\n",
      "2025-11-13 01:57:19,262 - bioneuralnet.downstream_task.dpmon - INFO - Starting DPMON run.\n",
      "2025-11-13 01:57:19,273 - bioneuralnet.downstream_task.dpmon - INFO - Running hyperparameter tuning for DPMON.\n",
      "2025-11-13 01:57:19,273 - bioneuralnet.downstream_task.dpmon - INFO - Using GPU 0\n",
      "2025-11-13 01:57:19,444 - bioneuralnet.downstream_task.dpmon - INFO - Number of nodes in network: 4498\n",
      "2025-11-13 01:57:21,431 - bioneuralnet.downstream_task.dpmon - INFO - Starting hyperparameter tuning for dataset shape: (511, 4499)\n",
      "2025-11-13 01:57:21,432\tINFO tune.py:616 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n",
      "2025-11-13 01:58:02,337\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/vicente/ray_results/tune_dp' in 0.0132s.\n",
      "2025-11-13 01:58:02,366 - bioneuralnet.downstream_task.dpmon - INFO - Best trial config: {'gnn_layer_num': 2, 'gnn_hidden_dim': 16, 'lr': 0.0027003956214294257, 'weight_decay': 0.0024031420469003027, 'nn_hidden_dim1': 64, 'nn_hidden_dim2': 8, 'num_epochs': 512}\n",
      "2025-11-13 01:58:02,366 - bioneuralnet.downstream_task.dpmon - INFO - Best trial final loss: 0.6153202056884766\n",
      "2025-11-13 01:58:02,367 - bioneuralnet.downstream_task.dpmon - INFO - Best trial final accuracy: 0.9980430528375733\n",
      "2025-11-13 01:58:02,370 - bioneuralnet.downstream_task.dpmon - INFO -    gnn_layer_num  gnn_hidden_dim      lr  weight_decay  nn_hidden_dim1  \\\n",
      "0              2              16  0.0027      0.002403              64   \n",
      "\n",
      "   nn_hidden_dim2  num_epochs  \n",
      "0               8         512  \n",
      "2025-11-13 01:58:02,372 - bioneuralnet.downstream_task.dpmon - INFO - Best tuned parameters: {'gnn_layer_num': 2, 'gnn_hidden_dim': 16, 'lr': 0.0027003956214294257, 'weight_decay': 0.0024031420469003027, 'nn_hidden_dim1': 64, 'nn_hidden_dim2': 8, 'num_epochs': 512}\n",
      "2025-11-13 01:58:02,372 - bioneuralnet.downstream_task.dpmon - INFO - Best tuned parameters: {'gnn_layer_num': 2, 'gnn_hidden_dim': 16, 'lr': 0.0027003956214294257, 'weight_decay': 0.0024031420469003027, 'nn_hidden_dim1': 64, 'nn_hidden_dim2': 8, 'num_epochs': 512}\n",
      "2025-11-13 01:58:02,372 - bioneuralnet.downstream_task.dpmon - INFO - Running standard training with tuned parameters.\n",
      "2025-11-13 01:58:02,372 - bioneuralnet.downstream_task.dpmon - INFO - Using GPU 0\n",
      "2025-11-13 01:58:02,546 - bioneuralnet.downstream_task.dpmon - INFO - Number of nodes in network: 4498\n",
      "2025-11-13 01:58:04,324 - bioneuralnet.downstream_task.dpmon - INFO - Training iteration 1/3\n",
      "2025-11-13 01:58:05,656 - bioneuralnet.downstream_task.dpmon - INFO - Training Accuracy: 0.7984\n",
      "2025-11-13 01:58:05,659 - bioneuralnet.downstream_task.dpmon - INFO - Model saved to /home/vicente/Github/BioNeuralNet/dpmon_results_GCN_FINAL/dpm_model_iter_1.pth\n",
      "2025-11-13 01:58:05,662 - bioneuralnet.downstream_task.dpmon - INFO - Training iteration 2/3\n",
      "2025-11-13 01:58:06,998 - bioneuralnet.downstream_task.dpmon - INFO - Training Accuracy: 0.9178\n",
      "2025-11-13 01:58:07,000 - bioneuralnet.downstream_task.dpmon - INFO - Model saved to /home/vicente/Github/BioNeuralNet/dpmon_results_GCN_FINAL/dpm_model_iter_2.pth\n",
      "2025-11-13 01:58:07,003 - bioneuralnet.downstream_task.dpmon - INFO - Training iteration 3/3\n",
      "2025-11-13 01:58:08,234 - bioneuralnet.downstream_task.dpmon - INFO - Training Accuracy: 0.9119\n",
      "2025-11-13 01:58:08,237 - bioneuralnet.downstream_task.dpmon - INFO - Model saved to /home/vicente/Github/BioNeuralNet/dpmon_results_GCN_FINAL/dpm_model_iter_3.pth\n",
      "2025-11-13 01:58:08,239 - bioneuralnet.downstream_task.dpmon - INFO - Best Accuracy: 0.9178\n",
      "2025-11-13 01:58:08,239 - bioneuralnet.downstream_task.dpmon - INFO - Average Accuracy across 3 models: 0.8761\n",
      "2025-11-13 01:58:08,239 - bioneuralnet.downstream_task.dpmon - INFO - Standard Deviation across all models: 0.0673\n",
      "2025-11-13 01:58:08,239 - bioneuralnet.downstream_task.dpmon - INFO - Returning best model predictions and average accuracy (predictions, avg_accuracy).\n",
      "2025-11-13 01:58:08,242 - bioneuralnet.utils.data - INFO - Setting global seed for reproducibility to: 121\n",
      "2025-11-13 01:58:08,243 - bioneuralnet.utils.data - INFO - CUDA available. Applying seed to all GPU operations\n",
      "2025-11-13 01:58:08,244 - bioneuralnet.utils.data - INFO - Seed setting complete\n",
      "2025-11-13 01:58:08,244 - bioneuralnet.downstream_task.dpmon - INFO - Output directory set to: /home/vicente/Github/BioNeuralNet/dpmon_results_GCN_FINAL\n",
      "2025-11-13 01:58:08,245 - bioneuralnet.downstream_task.dpmon - INFO - Initialized DPMON with the provided parameters.\n",
      "2025-11-13 01:58:08,245 - bioneuralnet.downstream_task.dpmon - INFO - Starting DPMON run.\n",
      "2025-11-13 01:58:08,263 - bioneuralnet.downstream_task.dpmon - INFO - Running hyperparameter tuning for DPMON.\n",
      "2025-11-13 01:58:08,263 - bioneuralnet.downstream_task.dpmon - INFO - Using GPU 0\n",
      "2025-11-13 01:58:08,453 - bioneuralnet.downstream_task.dpmon - INFO - Number of nodes in network: 4498\n",
      "2025-11-13 01:58:10,367 - bioneuralnet.downstream_task.dpmon - INFO - Starting hyperparameter tuning for dataset shape: (511, 4499)\n",
      "2025-11-13 01:58:10,367\tINFO tune.py:616 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n",
      "2025-11-13 01:58:47,261\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/vicente/ray_results/tune_dp' in 0.0028s.\n",
      "2025-11-13 01:58:47,285 - bioneuralnet.downstream_task.dpmon - INFO - Best trial config: {'gnn_layer_num': 8, 'gnn_hidden_dim': 128, 'lr': 0.00042884734295576757, 'weight_decay': 0.0004999049799078241, 'nn_hidden_dim1': 64, 'nn_hidden_dim2': 32, 'num_epochs': 8192}\n",
      "2025-11-13 01:58:47,285 - bioneuralnet.downstream_task.dpmon - INFO - Best trial final loss: 0.6262099146842957\n",
      "2025-11-13 01:58:47,286 - bioneuralnet.downstream_task.dpmon - INFO - Best trial final accuracy: 1.0\n",
      "2025-11-13 01:58:47,288 - bioneuralnet.downstream_task.dpmon - INFO -    gnn_layer_num  gnn_hidden_dim        lr  weight_decay  nn_hidden_dim1  \\\n",
      "0              8             128  0.000429        0.0005              64   \n",
      "\n",
      "   nn_hidden_dim2  num_epochs  \n",
      "0              32        8192  \n",
      "2025-11-13 01:58:47,290 - bioneuralnet.downstream_task.dpmon - INFO - Best tuned parameters: {'gnn_layer_num': 8, 'gnn_hidden_dim': 128, 'lr': 0.00042884734295576757, 'weight_decay': 0.0004999049799078241, 'nn_hidden_dim1': 64, 'nn_hidden_dim2': 32, 'num_epochs': 8192}\n",
      "2025-11-13 01:58:47,290 - bioneuralnet.downstream_task.dpmon - INFO - Best tuned parameters: {'gnn_layer_num': 8, 'gnn_hidden_dim': 128, 'lr': 0.00042884734295576757, 'weight_decay': 0.0004999049799078241, 'nn_hidden_dim1': 64, 'nn_hidden_dim2': 32, 'num_epochs': 8192}\n",
      "2025-11-13 01:58:47,290 - bioneuralnet.downstream_task.dpmon - INFO - Running standard training with tuned parameters.\n",
      "2025-11-13 01:58:47,291 - bioneuralnet.downstream_task.dpmon - INFO - Using GPU 0\n",
      "2025-11-13 01:58:47,467 - bioneuralnet.downstream_task.dpmon - INFO - Number of nodes in network: 4498\n",
      "2025-11-13 01:58:49,261 - bioneuralnet.downstream_task.dpmon - INFO - Training iteration 1/3\n",
      "2025-11-13 02:00:23,464 - bioneuralnet.downstream_task.dpmon - INFO - Training Accuracy: 1.0000\n",
      "2025-11-13 02:00:23,469 - bioneuralnet.downstream_task.dpmon - INFO - Model saved to /home/vicente/Github/BioNeuralNet/dpmon_results_GCN_FINAL/dpm_model_iter_1.pth\n",
      "2025-11-13 02:00:23,477 - bioneuralnet.downstream_task.dpmon - INFO - Training iteration 2/3\n",
      "2025-11-13 02:01:58,261 - bioneuralnet.downstream_task.dpmon - INFO - Training Accuracy: 1.0000\n",
      "2025-11-13 02:01:58,264 - bioneuralnet.downstream_task.dpmon - INFO - Model saved to /home/vicente/Github/BioNeuralNet/dpmon_results_GCN_FINAL/dpm_model_iter_2.pth\n",
      "2025-11-13 02:01:58,271 - bioneuralnet.downstream_task.dpmon - INFO - Training iteration 3/3\n",
      "2025-11-13 02:03:32,932 - bioneuralnet.downstream_task.dpmon - INFO - Training Accuracy: 1.0000\n",
      "2025-11-13 02:03:32,935 - bioneuralnet.downstream_task.dpmon - INFO - Model saved to /home/vicente/Github/BioNeuralNet/dpmon_results_GCN_FINAL/dpm_model_iter_3.pth\n",
      "2025-11-13 02:03:32,942 - bioneuralnet.downstream_task.dpmon - INFO - Best Accuracy: 1.0000\n",
      "2025-11-13 02:03:32,942 - bioneuralnet.downstream_task.dpmon - INFO - Average Accuracy across 3 models: 1.0000\n",
      "2025-11-13 02:03:32,942 - bioneuralnet.downstream_task.dpmon - INFO - Standard Deviation across all models: 0.0000\n",
      "2025-11-13 02:03:32,942 - bioneuralnet.downstream_task.dpmon - INFO - Returning best model predictions and average accuracy (predictions, avg_accuracy).\n",
      "2025-11-13 02:03:32,945 - bioneuralnet.utils.data - INFO - Setting global seed for reproducibility to: 122\n",
      "2025-11-13 02:03:32,946 - bioneuralnet.utils.data - INFO - CUDA available. Applying seed to all GPU operations\n",
      "2025-11-13 02:03:32,946 - bioneuralnet.utils.data - INFO - Seed setting complete\n",
      "2025-11-13 02:03:32,947 - bioneuralnet.downstream_task.dpmon - INFO - Output directory set to: /home/vicente/Github/BioNeuralNet/dpmon_results_GCN_FINAL\n",
      "2025-11-13 02:03:32,947 - bioneuralnet.downstream_task.dpmon - INFO - Initialized DPMON with the provided parameters.\n",
      "2025-11-13 02:03:32,947 - bioneuralnet.downstream_task.dpmon - INFO - Starting DPMON run.\n",
      "2025-11-13 02:03:32,957 - bioneuralnet.downstream_task.dpmon - INFO - Running hyperparameter tuning for DPMON.\n",
      "2025-11-13 02:03:32,957 - bioneuralnet.downstream_task.dpmon - INFO - Using GPU 0\n",
      "2025-11-13 02:03:33,137 - bioneuralnet.downstream_task.dpmon - INFO - Number of nodes in network: 4498\n",
      "2025-11-13 02:03:35,033 - bioneuralnet.downstream_task.dpmon - INFO - Starting hyperparameter tuning for dataset shape: (511, 4499)\n",
      "2025-11-13 02:03:35,034\tINFO tune.py:616 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
     ]
    }
   ],
   "source": [
    "output_dir_base_gcn = Path(\"/home/vicente/Github/BioNeuralNet/dpmon_results_GCN_FINAL\")\n",
    "n_repeats = 5\n",
    "all_preds = []\n",
    "\n",
    "for r in range(n_repeats):\n",
    "    bnn.utils.set_seed(SEED+r)\n",
    "    dpmon_repeat = DPMON(\n",
    "        adjacency_matrix=A_train,\n",
    "        omics_list=[dna_meth, rna, mirna],\n",
    "        phenotype_data=target,\n",
    "        clinical_data=clinical_preprocessed,\n",
    "        repeat_num=3,\n",
    "        model='GCN',\n",
    "        tune=True,\n",
    "        gpu=True,\n",
    "        cuda=0,\n",
    "        output_dir=output_dir_base_gcn,\n",
    "    )\n",
    "    \n",
    "    predictions_df, _ = dpmon_repeat.run()\n",
    "    all_preds.append(predictions_df[\"Predicted\"].values)\n",
    "\n",
    "all_preds = np.array(all_preds)\n",
    "\n",
    "f1_macro_list = [f1_score(target, pred, average='macro') for pred in all_preds]\n",
    "f1_weighted_list = [f1_score(target, pred, average='weighted') for pred in all_preds]\n",
    "accuracy_list = [accuracy_score(target, pred) for pred in all_preds]\n",
    "\n",
    "avg_f1_macro = np.mean(f1_macro_list)\n",
    "std_f1_macro = np.std(f1_macro_list)\n",
    "\n",
    "avg_f1_weighted = np.mean(f1_weighted_list)\n",
    "std_f1_weighted = np.std(f1_weighted_list)\n",
    "\n",
    "avg_acc = np.mean(accuracy_list)\n",
    "std_acc = np.std(accuracy_list)\n",
    "\n",
    "print(f\"Accuracy: {avg_acc:.4f} +/- {std_acc:.4f}\")\n",
    "print(f\"F1 Weighted: {avg_f1_weighted:.4f} +/- {std_f1_weighted:.4f}\")\n",
    "print(f\"F1 Macro: {avg_f1_macro:.4f} +/- {std_f1_macro:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a40cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-09 20:00:27,319 - bioneuralnet.utils.data - INFO - Setting global seed for reproducibility to: 118\n",
      "2025-11-09 20:00:27,319 - bioneuralnet.utils.data - INFO - CUDA available. Applying seed to all GPU operations\n",
      "2025-11-09 20:00:27,319 - bioneuralnet.utils.data - INFO - Seed setting complete\n",
      "2025-11-09 20:00:27,320 - bioneuralnet.downstream_task.dpmon - INFO - Output directory set to: /home/vicente/Github/BioNeuralNet/dpmon_cv_results_GAT_FINAL\n",
      "2025-11-09 20:00:27,320 - bioneuralnet.downstream_task.dpmon - INFO - Initialized DPMON with the provided parameters.\n",
      "2025-11-09 20:00:27,320 - bioneuralnet.downstream_task.dpmon - INFO - Starting DPMON run.\n",
      "2025-11-09 20:00:27,330 - bioneuralnet.downstream_task.dpmon - INFO - Running hyperparameter tuning for DPMON.\n",
      "2025-11-09 20:00:27,331 - bioneuralnet.downstream_task.dpmon - INFO - Using GPU 0\n",
      "2025-11-09 20:00:27,498 - bioneuralnet.downstream_task.dpmon - INFO - Number of nodes in network: 4498\n",
      "2025-11-09 20:00:30,311 - bioneuralnet.downstream_task.dpmon - INFO - Starting hyperparameter tuning for dataset shape: (511, 4499)\n",
      "2025-11-09 20:00:30,312\tINFO tune.py:616 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n",
      "2025-11-09 20:01:06,760\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/vicente/ray_results/tune_dp' in 0.0027s.\n",
      "2025-11-09 20:01:06,778 - bioneuralnet.downstream_task.dpmon - INFO - Best trial config: {'gnn_layer_num': 8, 'gnn_hidden_dim': 32, 'lr': 0.00040074470948823875, 'weight_decay': 0.007822838314187376, 'nn_hidden_dim1': 32, 'nn_hidden_dim2': 32, 'num_epochs': 2048}\n",
      "2025-11-09 20:01:06,778 - bioneuralnet.downstream_task.dpmon - INFO - Best trial final loss: 0.6445675492286682\n",
      "2025-11-09 20:01:06,779 - bioneuralnet.downstream_task.dpmon - INFO - Best trial final accuracy: 1.0\n",
      "2025-11-09 20:01:06,781 - bioneuralnet.downstream_task.dpmon - INFO -    gnn_layer_num  gnn_hidden_dim        lr  weight_decay  nn_hidden_dim1  \\\n",
      "0              8              32  0.000401      0.007823              32   \n",
      "\n",
      "   nn_hidden_dim2  num_epochs  \n",
      "0              32        2048  \n",
      "2025-11-09 20:01:06,783 - bioneuralnet.downstream_task.dpmon - INFO - Best tuned parameters: {'gnn_layer_num': 8, 'gnn_hidden_dim': 32, 'lr': 0.00040074470948823875, 'weight_decay': 0.007822838314187376, 'nn_hidden_dim1': 32, 'nn_hidden_dim2': 32, 'num_epochs': 2048}\n",
      "2025-11-09 20:01:06,784 - bioneuralnet.downstream_task.dpmon - INFO - Best tuned parameters: {'gnn_layer_num': 8, 'gnn_hidden_dim': 32, 'lr': 0.00040074470948823875, 'weight_decay': 0.007822838314187376, 'nn_hidden_dim1': 32, 'nn_hidden_dim2': 32, 'num_epochs': 2048}\n",
      "2025-11-09 20:01:06,784 - bioneuralnet.downstream_task.dpmon - INFO - Running standard training with tuned parameters.\n",
      "2025-11-09 20:01:06,784 - bioneuralnet.downstream_task.dpmon - INFO - Using GPU 0\n",
      "2025-11-09 20:01:06,953 - bioneuralnet.downstream_task.dpmon - INFO - Number of nodes in network: 4498\n",
      "2025-11-09 20:01:09,771 - bioneuralnet.downstream_task.dpmon - INFO - Training iteration 1/3\n",
      "2025-11-09 20:01:19,774 - bioneuralnet.downstream_task.dpmon - INFO - Training Accuracy: 1.0000\n",
      "2025-11-09 20:01:19,777 - bioneuralnet.downstream_task.dpmon - INFO - Model saved to /home/vicente/Github/BioNeuralNet/dpmon_cv_results_GAT_FINAL/dpm_model_iter_1.pth\n",
      "2025-11-09 20:01:19,779 - bioneuralnet.downstream_task.dpmon - INFO - Training iteration 2/3\n",
      "2025-11-09 20:01:29,857 - bioneuralnet.downstream_task.dpmon - INFO - Training Accuracy: 1.0000\n",
      "2025-11-09 20:01:29,860 - bioneuralnet.downstream_task.dpmon - INFO - Model saved to /home/vicente/Github/BioNeuralNet/dpmon_cv_results_GAT_FINAL/dpm_model_iter_2.pth\n",
      "2025-11-09 20:01:29,862 - bioneuralnet.downstream_task.dpmon - INFO - Training iteration 3/3\n",
      "2025-11-09 20:01:40,092 - bioneuralnet.downstream_task.dpmon - INFO - Training Accuracy: 1.0000\n",
      "2025-11-09 20:01:40,094 - bioneuralnet.downstream_task.dpmon - INFO - Model saved to /home/vicente/Github/BioNeuralNet/dpmon_cv_results_GAT_FINAL/dpm_model_iter_3.pth\n",
      "2025-11-09 20:01:40,097 - bioneuralnet.downstream_task.dpmon - INFO - Best Accuracy: 1.0000\n",
      "2025-11-09 20:01:40,097 - bioneuralnet.downstream_task.dpmon - INFO - Average Accuracy across 3 models: 1.0000\n",
      "2025-11-09 20:01:40,097 - bioneuralnet.downstream_task.dpmon - INFO - Standard Deviation across all models: 0.0000\n",
      "2025-11-09 20:01:40,097 - bioneuralnet.downstream_task.dpmon - INFO - Returning best model predictions and average accuracy (predictions, avg_accuracy).\n",
      "2025-11-09 20:01:40,100 - bioneuralnet.utils.data - INFO - Setting global seed for reproducibility to: 119\n",
      "2025-11-09 20:01:40,101 - bioneuralnet.utils.data - INFO - CUDA available. Applying seed to all GPU operations\n",
      "2025-11-09 20:01:40,101 - bioneuralnet.utils.data - INFO - Seed setting complete\n",
      "2025-11-09 20:01:40,101 - bioneuralnet.downstream_task.dpmon - INFO - Output directory set to: /home/vicente/Github/BioNeuralNet/dpmon_cv_results_GAT_FINAL\n",
      "2025-11-09 20:01:40,101 - bioneuralnet.downstream_task.dpmon - INFO - Initialized DPMON with the provided parameters.\n",
      "2025-11-09 20:01:40,101 - bioneuralnet.downstream_task.dpmon - INFO - Starting DPMON run.\n",
      "2025-11-09 20:01:40,111 - bioneuralnet.downstream_task.dpmon - INFO - Running hyperparameter tuning for DPMON.\n",
      "2025-11-09 20:01:40,111 - bioneuralnet.downstream_task.dpmon - INFO - Using GPU 0\n",
      "2025-11-09 20:01:40,454 - bioneuralnet.downstream_task.dpmon - INFO - Number of nodes in network: 4498\n",
      "2025-11-09 20:01:43,349 - bioneuralnet.downstream_task.dpmon - INFO - Starting hyperparameter tuning for dataset shape: (511, 4499)\n",
      "2025-11-09 20:01:43,349\tINFO tune.py:616 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n",
      "2025-11-09 20:02:16,776\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/vicente/ray_results/tune_dp' in 0.0023s.\n",
      "2025-11-09 20:02:16,795 - bioneuralnet.downstream_task.dpmon - INFO - Best trial config: {'gnn_layer_num': 8, 'gnn_hidden_dim': 16, 'lr': 0.0031152551982754385, 'weight_decay': 0.0004093955830993196, 'nn_hidden_dim1': 64, 'nn_hidden_dim2': 64, 'num_epochs': 4096}\n",
      "2025-11-09 20:02:16,795 - bioneuralnet.downstream_task.dpmon - INFO - Best trial final loss: 0.5520046353340149\n",
      "2025-11-09 20:02:16,796 - bioneuralnet.downstream_task.dpmon - INFO - Best trial final accuracy: 1.0\n",
      "2025-11-09 20:02:16,800 - bioneuralnet.downstream_task.dpmon - INFO -    gnn_layer_num  gnn_hidden_dim        lr  weight_decay  nn_hidden_dim1  \\\n",
      "0              8              16  0.003115      0.000409              64   \n",
      "\n",
      "   nn_hidden_dim2  num_epochs  \n",
      "0              64        4096  \n",
      "2025-11-09 20:02:16,801 - bioneuralnet.downstream_task.dpmon - INFO - Best tuned parameters: {'gnn_layer_num': 8, 'gnn_hidden_dim': 16, 'lr': 0.0031152551982754385, 'weight_decay': 0.0004093955830993196, 'nn_hidden_dim1': 64, 'nn_hidden_dim2': 64, 'num_epochs': 4096}\n",
      "2025-11-09 20:02:16,802 - bioneuralnet.downstream_task.dpmon - INFO - Best tuned parameters: {'gnn_layer_num': 8, 'gnn_hidden_dim': 16, 'lr': 0.0031152551982754385, 'weight_decay': 0.0004093955830993196, 'nn_hidden_dim1': 64, 'nn_hidden_dim2': 64, 'num_epochs': 4096}\n",
      "2025-11-09 20:02:16,802 - bioneuralnet.downstream_task.dpmon - INFO - Running standard training with tuned parameters.\n",
      "2025-11-09 20:02:16,802 - bioneuralnet.downstream_task.dpmon - INFO - Using GPU 0\n",
      "2025-11-09 20:02:16,970 - bioneuralnet.downstream_task.dpmon - INFO - Number of nodes in network: 4498\n",
      "2025-11-09 20:02:19,821 - bioneuralnet.downstream_task.dpmon - INFO - Training iteration 1/3\n",
      "2025-11-09 20:02:34,745 - bioneuralnet.downstream_task.dpmon - INFO - Training Accuracy: 0.7573\n",
      "2025-11-09 20:02:34,757 - bioneuralnet.downstream_task.dpmon - INFO - Model saved to /home/vicente/Github/BioNeuralNet/dpmon_cv_results_GAT_FINAL/dpm_model_iter_1.pth\n",
      "2025-11-09 20:02:34,760 - bioneuralnet.downstream_task.dpmon - INFO - Training iteration 2/3\n",
      "2025-11-09 20:02:49,927 - bioneuralnet.downstream_task.dpmon - INFO - Training Accuracy: 0.6810\n",
      "2025-11-09 20:02:49,939 - bioneuralnet.downstream_task.dpmon - INFO - Model saved to /home/vicente/Github/BioNeuralNet/dpmon_cv_results_GAT_FINAL/dpm_model_iter_2.pth\n",
      "2025-11-09 20:02:49,942 - bioneuralnet.downstream_task.dpmon - INFO - Training iteration 3/3\n",
      "2025-11-09 20:03:05,083 - bioneuralnet.downstream_task.dpmon - INFO - Training Accuracy: 0.9980\n",
      "2025-11-09 20:03:05,085 - bioneuralnet.downstream_task.dpmon - INFO - Model saved to /home/vicente/Github/BioNeuralNet/dpmon_cv_results_GAT_FINAL/dpm_model_iter_3.pth\n",
      "2025-11-09 20:03:05,087 - bioneuralnet.downstream_task.dpmon - INFO - Best Accuracy: 0.9980\n",
      "2025-11-09 20:03:05,088 - bioneuralnet.downstream_task.dpmon - INFO - Average Accuracy across 3 models: 0.8121\n",
      "2025-11-09 20:03:05,088 - bioneuralnet.downstream_task.dpmon - INFO - Standard Deviation across all models: 0.1655\n",
      "2025-11-09 20:03:05,088 - bioneuralnet.downstream_task.dpmon - INFO - Returning best model predictions and average accuracy (predictions, avg_accuracy).\n",
      "2025-11-09 20:03:05,090 - bioneuralnet.utils.data - INFO - Setting global seed for reproducibility to: 120\n",
      "2025-11-09 20:03:05,091 - bioneuralnet.utils.data - INFO - CUDA available. Applying seed to all GPU operations\n",
      "2025-11-09 20:03:05,091 - bioneuralnet.utils.data - INFO - Seed setting complete\n",
      "2025-11-09 20:03:05,091 - bioneuralnet.downstream_task.dpmon - INFO - Output directory set to: /home/vicente/Github/BioNeuralNet/dpmon_cv_results_GAT_FINAL\n",
      "2025-11-09 20:03:05,091 - bioneuralnet.downstream_task.dpmon - INFO - Initialized DPMON with the provided parameters.\n",
      "2025-11-09 20:03:05,092 - bioneuralnet.downstream_task.dpmon - INFO - Starting DPMON run.\n",
      "2025-11-09 20:03:05,100 - bioneuralnet.downstream_task.dpmon - INFO - Running hyperparameter tuning for DPMON.\n",
      "2025-11-09 20:03:05,101 - bioneuralnet.downstream_task.dpmon - INFO - Using GPU 0\n",
      "2025-11-09 20:03:05,272 - bioneuralnet.downstream_task.dpmon - INFO - Number of nodes in network: 4498\n",
      "2025-11-09 20:03:08,098 - bioneuralnet.downstream_task.dpmon - INFO - Starting hyperparameter tuning for dataset shape: (511, 4499)\n",
      "2025-11-09 20:03:08,099\tINFO tune.py:616 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n",
      "2025-11-09 20:03:43,957\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/vicente/ray_results/tune_dp' in 0.0116s.\n",
      "2025-11-09 20:03:43,980 - bioneuralnet.downstream_task.dpmon - INFO - Best trial config: {'gnn_layer_num': 2, 'gnn_hidden_dim': 16, 'lr': 0.0027003956214294257, 'weight_decay': 0.0024031420469003027, 'nn_hidden_dim1': 64, 'nn_hidden_dim2': 8, 'num_epochs': 512}\n",
      "2025-11-09 20:03:43,981 - bioneuralnet.downstream_task.dpmon - INFO - Best trial final loss: 0.6013627052307129\n",
      "2025-11-09 20:03:43,982 - bioneuralnet.downstream_task.dpmon - INFO - Best trial final accuracy: 1.0\n",
      "2025-11-09 20:03:43,987 - bioneuralnet.downstream_task.dpmon - INFO -    gnn_layer_num  gnn_hidden_dim      lr  weight_decay  nn_hidden_dim1  \\\n",
      "0              2              16  0.0027      0.002403              64   \n",
      "\n",
      "   nn_hidden_dim2  num_epochs  \n",
      "0               8         512  \n",
      "2025-11-09 20:03:43,989 - bioneuralnet.downstream_task.dpmon - INFO - Best tuned parameters: {'gnn_layer_num': 2, 'gnn_hidden_dim': 16, 'lr': 0.0027003956214294257, 'weight_decay': 0.0024031420469003027, 'nn_hidden_dim1': 64, 'nn_hidden_dim2': 8, 'num_epochs': 512}\n",
      "2025-11-09 20:03:43,989 - bioneuralnet.downstream_task.dpmon - INFO - Best tuned parameters: {'gnn_layer_num': 2, 'gnn_hidden_dim': 16, 'lr': 0.0027003956214294257, 'weight_decay': 0.0024031420469003027, 'nn_hidden_dim1': 64, 'nn_hidden_dim2': 8, 'num_epochs': 512}\n",
      "2025-11-09 20:03:43,989 - bioneuralnet.downstream_task.dpmon - INFO - Running standard training with tuned parameters.\n",
      "2025-11-09 20:03:43,989 - bioneuralnet.downstream_task.dpmon - INFO - Using GPU 0\n",
      "2025-11-09 20:03:44,157 - bioneuralnet.downstream_task.dpmon - INFO - Number of nodes in network: 4498\n",
      "2025-11-09 20:03:46,988 - bioneuralnet.downstream_task.dpmon - INFO - Training iteration 1/3\n",
      "2025-11-09 20:03:48,929 - bioneuralnet.downstream_task.dpmon - INFO - Training Accuracy: 0.9961\n",
      "2025-11-09 20:03:48,931 - bioneuralnet.downstream_task.dpmon - INFO - Model saved to /home/vicente/Github/BioNeuralNet/dpmon_cv_results_GAT_FINAL/dpm_model_iter_1.pth\n",
      "2025-11-09 20:03:48,933 - bioneuralnet.downstream_task.dpmon - INFO - Training iteration 2/3\n",
      "2025-11-09 20:03:50,821 - bioneuralnet.downstream_task.dpmon - INFO - Training Accuracy: 0.8963\n",
      "2025-11-09 20:03:50,824 - bioneuralnet.downstream_task.dpmon - INFO - Model saved to /home/vicente/Github/BioNeuralNet/dpmon_cv_results_GAT_FINAL/dpm_model_iter_2.pth\n",
      "2025-11-09 20:03:50,826 - bioneuralnet.downstream_task.dpmon - INFO - Training iteration 3/3\n",
      "2025-11-09 20:03:52,771 - bioneuralnet.downstream_task.dpmon - INFO - Training Accuracy: 1.0000\n",
      "2025-11-09 20:03:52,774 - bioneuralnet.downstream_task.dpmon - INFO - Model saved to /home/vicente/Github/BioNeuralNet/dpmon_cv_results_GAT_FINAL/dpm_model_iter_3.pth\n",
      "2025-11-09 20:03:52,776 - bioneuralnet.downstream_task.dpmon - INFO - Best Accuracy: 1.0000\n",
      "2025-11-09 20:03:52,776 - bioneuralnet.downstream_task.dpmon - INFO - Average Accuracy across 3 models: 0.9641\n",
      "2025-11-09 20:03:52,776 - bioneuralnet.downstream_task.dpmon - INFO - Standard Deviation across all models: 0.0588\n",
      "2025-11-09 20:03:52,776 - bioneuralnet.downstream_task.dpmon - INFO - Returning best model predictions and average accuracy (predictions, avg_accuracy).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9993 +/- 0.0009\n",
      "F1 Weighted: 0.9993 +/- 0.0009\n",
      "F1 Macro: 0.9993 +/- 0.0010\n"
     ]
    }
   ],
   "source": [
    "output_dir_base_gat = Path(\"/home/vicente/Github/BioNeuralNet/dpmon_results_GAT_FINAL\")\n",
    "\n",
    "n_repeats = 5\n",
    "all_preds = []\n",
    "\n",
    "for r in range(n_repeats):\n",
    "    bnn.utils.set_seed(SEED+r)\n",
    "    dpmon_repeat = DPMON(\n",
    "        adjacency_matrix=A_train,\n",
    "        omics_list=[dna_meth, rna, mirna],\n",
    "        phenotype_data=target,\n",
    "        clinical_data=clinical_preprocessed,\n",
    "        repeat_num=3,\n",
    "        model='GAT',\n",
    "        tune=True,\n",
    "        gpu=True,\n",
    "        cuda=0,\n",
    "        output_dir=output_dir_base_gat,\n",
    "    )\n",
    "    \n",
    "    predictions_df, _ = dpmon_repeat.run()\n",
    "    all_preds.append(predictions_df[\"Predicted\"].values)\n",
    "\n",
    "all_preds = np.array(all_preds)\n",
    "\n",
    "f1_macro_list = [f1_score(target, pred, average='macro') for pred in all_preds]\n",
    "f1_weighted_list = [f1_score(target, pred, average='weighted') for pred in all_preds]\n",
    "accuracy_list = [accuracy_score(target, pred) for pred in all_preds]\n",
    "\n",
    "avg_f1_macro = np.mean(f1_macro_list)\n",
    "std_f1_macro = np.std(f1_macro_list)\n",
    "\n",
    "avg_f1_weighted = np.mean(f1_weighted_list)\n",
    "std_f1_weighted = np.std(f1_weighted_list)\n",
    "\n",
    "avg_acc = np.mean(accuracy_list)\n",
    "std_acc = np.std(accuracy_list)\n",
    "\n",
    "print(f\"Accuracy: {avg_acc:.4f} +/- {std_acc:.4f}\")\n",
    "print(f\"F1 Weighted: {avg_f1_weighted:.4f} +/- {std_f1_weighted:.4f}\")\n",
    "print(f\"F1 Macro: {avg_f1_macro:.4f} +/- {std_f1_macro:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00140d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from scipy.stats import loguniform, randint\n",
    "\n",
    "X = pd.concat([dna_meth, rna, mirna, clinical_preprocessed], axis=1)\n",
    "y = target['phenotype']\n",
    "print(f\"Successfully created X matrix with shape: {X.shape}\")\n",
    "print(f\"Successfully created y vector with shape: {y.shape}\")\n",
    "\n",
    "all_results = {\n",
    "    \"LogisticRegression\": {\"acc\": [], \"f1_w\": [], \"f1_m\": []},\n",
    "    \"MLP\": {\"acc\": [], \"f1_w\": [], \"f1_m\": []},\n",
    "    \"XGBoost\": {\"acc\": [], \"f1_w\": [], \"f1_m\": []},\n",
    "}\n",
    "\n",
    "all_results = {\n",
    "    \"LogisticRegression\": {\"acc\": [], \"f1_w\": [], \"f1_m\": []},\n",
    "    \"MLP\": {\"acc\": [], \"f1_w\": [], \"f1_m\": []},\n",
    "    \"XGBoost\": {\"acc\": [], \"f1_w\": [], \"f1_m\": []},\n",
    "}\n",
    "\n",
    "N_REPEATS = 5\n",
    "TEST_SPLIT_SIZE = .7\n",
    "CV_FOLDS = 3\n",
    "N_ITER_SEARCH = 10\n",
    "\n",
    "pipe_lr = Pipeline([('scaler', StandardScaler()),\n",
    "    ('model', LogisticRegression(\n",
    "        solver='lbfgs',\n",
    "        max_iter=1000,\n",
    "        penalty=None \n",
    "    ))\n",
    "])\n",
    "\n",
    "pipe_mlp = Pipeline([('scaler', StandardScaler()),\n",
    "    ('model', MLPClassifier(\n",
    "        max_iter=500,\n",
    "        early_stopping=True,\n",
    "        n_iter_no_change=10\n",
    "    ))\n",
    "])\n",
    "\n",
    "pipe_xgb = Pipeline([('scaler', StandardScaler()),\n",
    "    ('model', XGBClassifier(\n",
    "        eval_metric='logloss'\n",
    "    ))\n",
    "])\n",
    "\n",
    "params_lr = {\n",
    "    'model__penalty': ['l2'], \n",
    "    'model__C': loguniform(1e-4, 1e2)\n",
    "}\n",
    "\n",
    "params_mlp = {\n",
    "    'model__hidden_layer_sizes': [(100,), (100, 50), (50, 50)],\n",
    "    'model__activation': ['relu', 'tanh'],\n",
    "    'model__alpha': loguniform(1e-5, 1e-1),\n",
    "    'model__learning_rate_init': loguniform(1e-4, 1e-2)\n",
    "}\n",
    "\n",
    "params_xgb = {\n",
    "    'model__n_estimators': randint(100, 500),\n",
    "    'model__learning_rate': loguniform(0.01, 0.3),\n",
    "    'model__max_depth': randint(3, 10),\n",
    "    'model__subsample': [0.7, 0.8, 0.9, 1.0],\n",
    "    'model__colsample_bytree': [0.7, 0.8, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "models_to_tune = {\n",
    "    \"LogisticRegression\": (pipe_lr, params_lr),\n",
    "    \"MLP\": (pipe_mlp, params_mlp),\n",
    "    \"XGBoost\": (pipe_xgb, params_xgb)\n",
    "}\n",
    "\n",
    "for r in range(N_REPEATS):\n",
    "    seed = SEED + r\n",
    "    print(f\"\\nRunning Repeat {r+1}/{N_REPEATS} (Seed: {seed})\")\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, \n",
    "        test_size=TEST_SPLIT_SIZE, \n",
    "        random_state=seed, \n",
    "        stratify=y)\n",
    "    \n",
    "    cv_splitter = StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=seed)\n",
    "    for name, (pipeline, params) in models_to_tune.items():\n",
    "        print(f\"Tuning {name}\")\n",
    "        search = RandomizedSearchCV(\n",
    "            estimator=pipeline,\n",
    "            param_distributions=params,\n",
    "            n_iter=N_ITER_SEARCH,\n",
    "            cv=cv_splitter,\n",
    "            scoring='f1_weighted',\n",
    "            n_jobs=-1,\n",
    "            random_state=seed\n",
    "        )\n",
    "        \n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "            warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "            search.fit(X_train, y_train)\n",
    "\n",
    "        print(f\"Best params for {name}: {search.best_params_}\")\n",
    "        \n",
    "        best_model = search.best_estimator_\n",
    "        preds = best_model.predict(X_test)\n",
    "\n",
    "        acc = accuracy_score(y_test, preds)\n",
    "        f1_w = f1_score(y_test, preds, average='weighted', zero_division=0)\n",
    "        f1_m = f1_score(y_test, preds, average='macro', zero_division=0)\n",
    "\n",
    "        all_results[name][\"acc\"].append(acc)\n",
    "        all_results[name][\"f1_w\"].append(f1_w)\n",
    "        all_results[name][\"f1_m\"].append(f1_m)\n",
    "\n",
    "print(f\"Tuned Model Results (Averaged over {N_REPEATS} runs)\")\n",
    "print(f\"(Tuning was {N_ITER_SEARCH} iterations with {CV_FOLDS}fold CV)\")\n",
    "\n",
    "for model_name, metrics in all_results.items():\n",
    "    avg_acc = np.mean(metrics[\"acc\"])\n",
    "    std_acc = np.std(metrics[\"acc\"])\n",
    "    \n",
    "    avg_f1_w = np.mean(metrics[\"f1_w\"])\n",
    "    std_f1_w = np.std(metrics[\"f1_w\"])\n",
    "    \n",
    "    avg_f1_m = np.mean(metrics[\"f1_m\"])\n",
    "    std_f1_m = np.std(metrics[\"f1_m\"])\n",
    "\n",
    "    print(f\"Results for {model_name}:\")\n",
    "    print(f\"Accuracy: {avg_acc:.4f} +/- {std_acc:.4f}\")\n",
    "    print(f\"F1 Weighted: {avg_f1_w:.4f} +/- {std_f1_w:.4f}\")\n",
    "    print(f\"F1 Macro: {avg_f1_m:.4f} +/- {std_f1_m:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0c1ea8",
   "metadata": {},
   "source": [
    "## values below are just place holders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a07947",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bioneuralnet as bnn\n",
    "\n",
    "gnn_plot_data = {\n",
    "    \"Accuracy\": {\n",
    "        \"SAGE\": (0.9903, 0.0173), \n",
    "        \"GCN\": (0.9720, 0.0379), \n",
    "        \"GAT\": (0.9626, 0.0575)\n",
    "    },\n",
    "    \"F1 Weighted\": {\n",
    "        \"SAGE\": (0.9903, 0.0173), \n",
    "        \"GCN\": (0.9631, 0.0557), \n",
    "        \"GAT\": (0.9546, 0.0732)\n",
    "    },\n",
    "    \"F1 Macro\": {\n",
    "        \"SAGE\": (0.9876, 0.0203), \n",
    "        \"GCN\": (0.9150, 0.1410), \n",
    "        \"GAT\": (0.9107, 0.1569)\n",
    "    }\n",
    "}\n",
    "\n",
    "baseline_plot_data = {\n",
    "    \"Accuracy\": {\n",
    "        \"SAGE\": (0.9903, 0.0173), \n",
    "        \"LogReg\": (0.9553, 0.0090), \n",
    "        \"XGBoost\": (0.9527, 0.0059), \n",
    "        \"MLP\": (0.9362, 0.0133)\n",
    "    },\n",
    "    \"F1 Weighted\": {\n",
    "        \"SAGE\": (0.9903, 0.0173), \n",
    "        \"LogReg\": (0.9557, 0.0088), \n",
    "        \"XGBoost\": (0.9529, 0.0058), \n",
    "        \"MLP\": (0.9379, 0.0125)\n",
    "    },\n",
    "    \"F1 Macro\": {\n",
    "        \"SAGE\": (0.9876, 0.0203), \n",
    "        \"LogReg\": (0.9413, 0.0124), \n",
    "        \"XGBoost\": (0.9451, 0.0106), \n",
    "        \"MLP\": (0.9138, 0.0170)\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "bnn.metrics.plot_multiple_metrics(\n",
    "    gnn_plot_data,\n",
    "    title_map={\n",
    "        \"Accuracy\": \"GNNs Comparison: Accuracy\",\n",
    "        \"F1 Weighted\": \"GNNs Comparison: F1 Weighted\",\n",
    "        \"F1 Macro\": \"GNNs Comparison: F1 Macro\"\n",
    "    }\n",
    ")\n",
    "\n",
    "bnn.metrics.plot_multiple_metrics(\n",
    "    baseline_plot_data,\n",
    "    title_map={\n",
    "        \"Accuracy\": \"SAGE vs. Baselines: Accuracy\",\n",
    "        \"F1 Weighted\": \"SAGE vs. Baselines: F1 Weighted\",\n",
    "        \"F1 Macro\": \"SAGE vs. Baselines: F1 Macro\"\n",
    "    }\n",
    ")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".enviroment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
